import asyncio
import argparse
from datetime import datetime
from typing import Optional, Dict, Any, List
import json
import os
import requests
from crawler.wechat import WeChatCrawler
from crawler.article_discovery import ArticleDiscovery
from crawler.history_crawler import HistoryCrawler
from crawler.series_crawler import SeriesCrawler
from storage.database import DatabaseManager
from storage.models import CrawlJob
from proxy.manager import ProxyManager
from utils.logger import logger
from utils.config import settings
from analytics.trend_analyzer import TrendAnalyzer
from analytics.tag_extractor import TagExtractor
from analytics.content_evaluator import ContentEvaluator


def _is_error_url(url: str) -> bool:
    """æ£€æµ‹æ˜¯å¦æ˜¯é”™è¯¯æˆ–å¼‚å¸¸URL"""
    error_indicators = [
        'wappoc_appmsgcaptcha',  # éªŒè¯ç é¡µé¢
        'mp_profile_redirect',   # é‡å®šå‘é”™è¯¯
        'error',                 # ä¸€èˆ¬é”™è¯¯é¡µé¢
        'blocked',               # è¢«å±è”½é¡µé¢
        'forbidden',             # ç¦æ­¢è®¿é—®
    ]
    
    for indicator in error_indicators:
        if indicator in url.lower():
            return True
    return False


def _is_error_title(title: str) -> bool:
    """æ£€æµ‹æ˜¯å¦æ˜¯é”™è¯¯æ ‡é¢˜"""
    if not title:
        return True
    
    error_titles = [
        'ç¯å¢ƒå¼‚å¸¸',
        'ç³»ç»Ÿé”™è¯¯', 
        'é¡µé¢ä¸å­˜åœ¨',
        'ç½‘ç»œé”™è¯¯',
        'éªŒè¯ç ',
        'Error',
        'Not Found'
    ]
    
    for error_title in error_titles:
        if error_title in title:
            return True
    return False


def _clean_article_content(content: str) -> str:
    """æ¸…ç†æ–‡ç« å†…å®¹ï¼Œå»é™¤æ¨¡æ¿æ–‡å­—å’Œå¹¿å‘Š"""
    if not content:
        return content
    
    import re
    
    # å®šä¹‰éœ€è¦åˆ é™¤çš„æ¨¡æ¿å†…å®¹
    template_patterns = [
        # Rebase ç›¸å…³çš„æ¨¡æ¿å†…å®¹
        r'å¾®ä¿¡ä¸æ”¯æŒå¤–éƒ¨é“¾æ¥.*?é˜…è¯»åŸæ–‡.*?æµè§ˆæ¯æœŸæ—¥æŠ¥å†…å®¹ã€‚',
        r'Web3 æå®¢æ—¥æŠ¥æ˜¯ä¸º Web3 æ—¶ä»£çš„æå®¢ä»¬å‡†å¤‡çš„æ—¥å¸¸è¯»ç‰©.*?å¹¶æ³¨æ˜æ—¥æŠ¥è´¡çŒ®ã€‚',
        r'ç½‘ç«™:\s*https://rebase\.network',
        r'å…¬ä¼—å·:\s*rebase_network',
        
        # é€šç”¨çš„å¾®ä¿¡å…¬ä¼—å·æ¨¡æ¿
        r'ç‚¹å‡».*?é˜…è¯»åŸæ–‡.*?æŸ¥çœ‹.*?',
        r'é•¿æŒ‰.*?è¯†åˆ«.*?äºŒç»´ç .*?å…³æ³¨',
        r'æ‰«æ.*?äºŒç»´ç .*?å…³æ³¨.*?å…¬ä¼—å·',
        r'å…³æ³¨.*?å…¬ä¼—å·.*?è·å–æ›´å¤š.*?',
        r'æ›´å¤š.*?å†…å®¹.*?è¯·å…³æ³¨.*?',
        r'æ¬¢è¿.*?è½¬å‘.*?åˆ†äº«.*?æœ‹å‹åœˆ',
        
        # å…¶ä»–å¸¸è§çš„ç»“å°¾æ¨¡æ¿
        r'æœ¬æ–‡.*?é¦–å‘.*?å…¬ä¼—å·.*?',
        r'åŸæ–‡é“¾æ¥.*?https?://[^\s]+',
        r'æ¥æº.*?https?://[^\s]+',
        r'å£°æ˜.*?æœ¬æ–‡.*?è½¬è½½.*?',
        r'å…è´£å£°æ˜.*?æŠ•èµ„æœ‰é£é™©.*?',
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œå’Œåˆ†éš”ç¬¦
        r'\n\s*\n\s*\n',  # å¤šä¸ªè¿ç»­ç©ºè¡Œ
        r'[=\-_]{3,}',    # è¿ç»­çš„åˆ†éš”ç¬¦
    ]
    
    cleaned_content = content
    
    # åº”ç”¨æ‰€æœ‰æ¸…ç†è§„åˆ™
    for pattern in template_patterns:
        cleaned_content = re.sub(pattern, '', cleaned_content, flags=re.DOTALL | re.IGNORECASE)
    
    # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
    cleaned_content = re.sub(r'\n\s*\n\s*\n', '\n\n', cleaned_content)  # å¤šä¸ªç©ºè¡Œå˜æˆä¸¤ä¸ª
    cleaned_content = re.sub(r'^\s+|\s+$', '', cleaned_content)  # å»é™¤é¦–å°¾ç©ºç™½
    
    return cleaned_content


def _extract_episode_from_title(title: str) -> str:
    """ä»æ ‡é¢˜ä¸­æå–æœŸæ•°"""
    import re
    
    # å°è¯•åŒ¹é…ä¸åŒæ ¼å¼çš„æœŸæ•°
    patterns = [
        r'#(\d+)',                    # #1350
        r'ç¬¬\s*(\d+)\s*æœŸ',           # ç¬¬1350æœŸ
        r'Vol\.?\s*(\d+)',            # Vol.1350 æˆ– Vol 1350
        r'\((\d+)\)',                 # (1350)
        r'ã€(\d+)ã€‘',                 # ã€1350ã€‘
    ]
    
    for pattern in patterns:
        match = re.search(pattern, title, re.IGNORECASE)
        if match:
            return f"#{match.group(1)}"
    
    return ""


def _get_content_summary(content: str, max_length: int = 200) -> str:
    """ç”Ÿæˆå†…å®¹æ‘˜è¦"""
    import re
    
    if not content:
        return ""
    
    # æ¸…ç†å†…å®¹
    cleaned = _clean_article_content(content)
    
    # ç§»é™¤æ¢è¡Œç¬¦ï¼Œä¿ç•™ç©ºæ ¼
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    
    # æˆªå–æ‘˜è¦
    if len(cleaned) <= max_length:
        return cleaned
    
    # åœ¨åˆé€‚çš„ä½ç½®æˆªæ–­ï¼ˆå¥å·ã€é—®å·ã€æ„Ÿå¹å·ï¼‰
    summary = cleaned[:max_length]
    
    # å°è¯•åœ¨æ ‡ç‚¹ç¬¦å·å¤„æˆªæ–­
    last_punct = max(
        summary.rfind('ã€‚'),
        summary.rfind('ï¼'),
        summary.rfind('ï¼Ÿ'),
        summary.rfind('.'),
        summary.rfind('!'),
        summary.rfind('?')
    )
    
    if last_punct > max_length * 0.7:  # å¦‚æœæ ‡ç‚¹åœ¨70%ä»¥åçš„ä½ç½®
        summary = summary[:last_punct + 1]
    else:
        summary += "..."
    
    return summary


def _format_article_to_structured(article: dict, article_id: int) -> dict:
    """å°†æ–‡ç« è½¬æ¢ä¸ºç»“æ„åŒ–æ ¼å¼"""
    import re
    
    title = article.get('title', '')
    content = article.get('content', '')
    
    # æå–æœŸæ•°
    episode = _extract_episode_from_title(title)
    
    # æ¸…ç†æ ‡é¢˜ï¼ˆå»é™¤æœŸæ•°éƒ¨åˆ†ï¼‰
    clean_title = title
    if episode:
        # ç§»é™¤æœŸæ•°ç›¸å…³çš„éƒ¨åˆ†
        patterns = [
            r'#\d+\s*',
            r'ç¬¬\s*\d+\s*æœŸ\s*',
            r'Vol\.?\s*\d+\s*',
            r'\(\d+\)\s*',
            r'ã€\d+ã€‘\s*',
        ]
        for pattern in patterns:
            clean_title = re.sub(pattern, '', clean_title, flags=re.IGNORECASE).strip()
    
    # è·å–å†…å®¹æ‘˜è¦
    introduce = _get_content_summary(content)
    
    # æ ¼å¼åŒ–æ—¶é—´
    publish_time = article.get('publish_time', '')
    if publish_time:
        try:
            # å¤„ç†ä¸åŒçš„æ—¶é—´æ ¼å¼
            for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d', '%Yå¹´%mæœˆ%dæ—¥']:
                try:
                    dt = datetime.strptime(publish_time.split(' ')[0], fmt)
                    publish_time = dt.strftime('%Y-%m-%d')
                    break
                except:
                    continue
        except:
            pass
    
    return {
        "id": article_id,
        "attributes": {
            "episode": episode,
            "title": clean_title,
            "author": article.get('author', article.get('account_name', 'Unknown')),
            "url": article.get('url', ''),
            "time": publish_time,
            "introduce": introduce,
            "full_content": _clean_article_content(content)  # ä¿ç•™å®Œæ•´å†…å®¹ä¾›éœ€è¦æ—¶ä½¿ç”¨
        }
    }


def _extract_daily_items_from_content(content: str) -> list:
    """ä»Web3æå®¢æ—¥æŠ¥å†…å®¹ä¸­æå–æ¯æ—¥æ¨èé¡¹ç›®"""
    items = []
    
    if not content:
        return items
    
    import re
    
    # æ¸…ç†å†…å®¹
    cleaned_content = _clean_article_content(content)
    
    # å°è¯•è¯†åˆ«æ—¥æŠ¥ä¸­çš„å„ä¸ªæ¨èé¡¹ç›®
    # é€šå¸¸æ ¼å¼ä¸ºï¼š1. æ ‡é¢˜ ä½œè€… é“¾æ¥ ä»‹ç»
    
    # åˆ†å‰²å†…å®¹ä¸ºæ®µè½
    paragraphs = cleaned_content.split('\n')
    
    current_item = {}
    item_number = 0
    
    for para in paragraphs:
        para = para.strip()
        if not para:
            continue
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„é¡¹ç›®å¼€å§‹ï¼ˆé€šå¸¸ä»¥æ•°å­—å¼€å¤´ï¼‰
        item_match = re.match(r'^(\d+)[\.\ã€]\s*(.+)', para)
        if item_match:
            # ä¿å­˜ä¹‹å‰çš„é¡¹ç›®
            if current_item and 'title' in current_item:
                items.append(current_item)
            
            item_number += 1
            remaining_text = item_match.group(2)
            
            # å°è¯•æå–æ ‡é¢˜ã€ä½œè€…ç­‰ä¿¡æ¯
            # æ ¼å¼å¯èƒ½æ˜¯ï¼šæ ‡é¢˜ by ä½œè€… æˆ– æ ‡é¢˜ï¼ˆä½œè€…ï¼‰
            by_match = re.search(r'^(.+?)\s+by\s+(\w+)', remaining_text, re.IGNORECASE)
            paren_match = re.search(r'^(.+?)\s*[ï¼ˆ(](\w+)[ï¼‰)]', remaining_text)
            
            if by_match:
                current_item = {
                    'number': item_number,
                    'title': by_match.group(1).strip(),
                    'author': by_match.group(2).strip()
                }
            elif paren_match:
                current_item = {
                    'number': item_number,
                    'title': paren_match.group(1).strip(),
                    'author': paren_match.group(2).strip()
                }
            else:
                current_item = {
                    'number': item_number,
                    'title': remaining_text.strip(),
                    'author': ''
                }
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯URL
        elif current_item and re.match(r'^https?://', para):
            current_item['url'] = para
        
        # å¦åˆ™å¯èƒ½æ˜¯ä»‹ç»æ–‡å­—
        elif current_item and 'title' in current_item:
            if 'introduce' not in current_item:
                current_item['introduce'] = para
            else:
                current_item['introduce'] += ' ' + para
    
    # ä¿å­˜æœ€åä¸€ä¸ªé¡¹ç›®
    if current_item and 'title' in current_item:
        items.append(current_item)
    
    return items


class GeekDailyAPI:
    """æå®¢æ—¥æŠ¥APIå®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str = "http://101.33.75.240:1337/api/v1/geekdailies"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'Content-Type': 'application/json',
            'Accept': 'application/json',
        })
    
    def fetch_page(self, page: int = 1, page_size: int = 100, sort: str = 'desc') -> Dict[str, Any]:
        """è·å–æŒ‡å®šé¡µé¢çš„æ•°æ®"""
        try:
            params = {
                'pagination[page]': page,
                'pagination[pageSize]': page_size,
                'sort': f'id:{sort}'
            }
            
            logger.info(f"è·å–ç¬¬ {page} é¡µæ•°æ® (æ¯é¡µ {page_size} æ¡)")
            response = self.session.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            
            return response.json()
            
        except requests.exceptions.RequestException as e:
            logger.error(f"APIè¯·æ±‚å¤±è´¥: {str(e)}")
            raise
    
    def fetch_all_pages(self, max_pages: Optional[int] = None, page_size: int = 100) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰é¡µé¢çš„æ•°æ®"""
        all_items = []
        
        try:
            # è·å–ç¬¬ä¸€é¡µè·å–æ€»é¡µæ•°
            first_page = self.fetch_page(1, page_size)
            all_items.extend(first_page.get('data', []))
            
            # è·å–åˆ†é¡µä¿¡æ¯
            pagination = first_page.get('meta', {}).get('pagination', {})
            total_pages = pagination.get('pageCount', 1)
            total_items = pagination.get('total', 0)
            
            logger.info(f"æ€»å…± {total_items} æ¡æ•°æ®ï¼Œ{total_pages} é¡µ")
            
            # é™åˆ¶æœ€å¤§é¡µæ•°
            if max_pages:
                total_pages = min(total_pages, max_pages)
                logger.info(f"é™åˆ¶è·å–å‰ {max_pages} é¡µ")
            
            # è·å–å‰©ä½™é¡µé¢
            if total_pages > 1:
                for page in range(2, total_pages + 1):
                    try:
                        page_data = self.fetch_page(page, page_size)
                        items = page_data.get('data', [])
                        all_items.extend(items)
                        logger.info(f"å·²è·å– {len(all_items)} / {total_items} æ¡æ•°æ®")
                        
                        # ç®€å•çš„å»¶è¿Ÿé¿å…è¿‡å¿«è¯·æ±‚
                        import time
                        time.sleep(0.5)
                        
                    except Exception as e:
                        logger.error(f"è·å–ç¬¬ {page} é¡µå¤±è´¥: {str(e)}")
                        continue
            
            logger.info(f"âœ… æˆåŠŸè·å– {len(all_items)} æ¡æ•°æ®")
            return all_items
            
        except Exception as e:
            logger.error(f"è·å–æ•°æ®å¤±è´¥: {str(e)}")
            raise


async def fetch_api_data(api_url: str = None, max_pages: Optional[int] = None, output_file: str = None):
    """ä»APIè·å–æ‰€æœ‰æ•°æ®å¹¶ä¿å­˜åˆ°æœ¬åœ°"""
    
    # è®¾ç½®é»˜è®¤å€¼
    if not api_url:
        api_url = "http://101.33.75.240:1337/api/v1/geekdailies"
    
    if not output_file:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f"data/api_geekdailies_{timestamp}.json"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    try:
        # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
        api_client = GeekDailyAPI(api_url)
        
        print(f"ğŸ“¡ å¼€å§‹ä»APIè·å–æ•°æ®: {api_url}")
        if max_pages:
            print(f"ğŸ“„ é™åˆ¶è·å–å‰ {max_pages} é¡µ")
        
        # è·å–æ‰€æœ‰æ•°æ®
        all_items = api_client.fetch_all_pages(max_pages)
        
        if not all_items:
            print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®")
            return
        
        # è½¬æ¢ä¸ºç»Ÿä¸€çš„ç»“æ„åŒ–æ ¼å¼
        structured_data = []
        
        for idx, item in enumerate(all_items, 1):
            try:
                # é€‚åº”APIæ•°æ®ç»“æ„
                attributes = item.get('attributes', {})
                
                structured_item = {
                    "id": item.get('id', idx),
                    "attributes": {
                        "episode": attributes.get('episode', ''),
                        "title": attributes.get('title', ''),
                        "author": attributes.get('author', ''),
                        "url": attributes.get('url', ''),
                        "time": attributes.get('time', ''),
                        "introduce": attributes.get('introduce', '')
                    }
                }
                
                structured_data.append(structured_item)
                
            except Exception as e:
                logger.warning(f"å¤„ç†ç¬¬ {idx} æ¡æ•°æ®æ—¶å‡ºé”™: {str(e)}")
                continue
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_data = {
            "export_info": {
                "source": "API",
                "api_url": api_url,
                "total": len(structured_data),
                "export_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "max_pages": max_pages
            },
            "articles": structured_data
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æˆåŠŸä¿å­˜ {len(structured_data)} æ¡æ•°æ®åˆ°: {output_file}")
        
        # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
        print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"  æ€»æ¡æ•°: {len(structured_data)}")
        
        # ç»Ÿè®¡ä½œè€…åˆ†å¸ƒ
        authors = {}
        for item in structured_data:
            author = item['attributes'].get('author', 'Unknown')
            authors[author] = authors.get(author, 0) + 1
        
        print(f"  ä½œè€…æ•°é‡: {len(authors)}")
        print(f"  çƒ­é—¨ä½œè€…:")
        for author, count in sorted(authors.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"    {author}: {count} ç¯‡")
        
        # ç»Ÿè®¡æ—¶é—´åˆ†å¸ƒ
        years = {}
        for item in structured_data:
            time_str = item['attributes'].get('time', '')
            if time_str:
                try:
                    year = time_str.split('-')[0]
                    years[year] = years.get(year, 0) + 1
                except:
                    pass
        
        if years:
            print(f"  æ—¶é—´åˆ†å¸ƒ:")
            for year, count in sorted(years.items(), reverse=True)[:5]:
                print(f"    {year}: {count} ç¯‡")
        
        return output_file
        
    except Exception as e:
        logger.error(f"ä»APIè·å–æ•°æ®å¤±è´¥: {str(e)}")
        print(f"âŒ è·å–æ•°æ®å¤±è´¥: {str(e)}")
        raise


async def merge_json_files(file1: str, file2: str, output_file: str = None, dedup_strategy: str = 'url', 
                          smart_split: bool = True, filter_bad_data: bool = True, fill_time: bool = True):
    """åˆå¹¶ä¸¤ä¸ªJSONæ–‡ä»¶å¹¶å»é‡"""
    
    try:
        # è¯»å–ç¬¬ä¸€ä¸ªæ–‡ä»¶
        print(f"ğŸ“– è¯»å–æ–‡ä»¶1: {file1}")
        with open(file1, 'r', encoding='utf-8') as f:
            data1 = json.load(f)
        
        # è¯»å–ç¬¬äºŒä¸ªæ–‡ä»¶
        print(f"ğŸ“– è¯»å–æ–‡ä»¶2: {file2}")
        with open(file2, 'r', encoding='utf-8') as f:
            data2 = json.load(f)
        
        # æå–æ–‡ç« æ•°æ®
        articles1 = data1.get('articles', [])
        articles2 = data2.get('articles', [])
        
        print(f"ğŸ“Š æ–‡ä»¶1åŒ…å« {len(articles1)} æ¡æ•°æ®")
        print(f"ğŸ“Š æ–‡ä»¶2åŒ…å« {len(articles2)} æ¡æ•°æ®")
        
        # åˆå¹¶æ•°æ®å¹¶åº”ç”¨æ™ºèƒ½å¤„ç†
        merged_articles = []
        seen = set()
        
        # ç»Ÿè®¡ä¿¡æ¯
        duplicates = 0
        split_count = 0
        filtered_count = 0
        time_filled_count = 0
        
        # è¾…åŠ©å‡½æ•°å®šä¹‰
        def safe_strip(value):
            """å®‰å…¨åœ°å¤„ç†å­—ç¬¦ä¸²stripï¼Œé¿å…Noneå€¼é”™è¯¯"""
            if value is None:
                return ''
            return str(value).strip()
        
        def parse_content_items(content_text):
            """è§£ææ–‡ç« å†…å®¹ï¼Œæå–ç‹¬ç«‹çš„æ–‡ç« æ¡ç›®"""
            if not content_text:
                return []
            
            import re
            items = []
            
            # ç”¨æ›´æ™ºèƒ½çš„æ–¹å¼åˆ‡åˆ†ï¼šæŒ‰ç…§ "æ ‡é¢˜\nURL\nä½œè€…:\nå†…å®¹" çš„æ¨¡å¼
            url_pattern = r'https?://[^\s]+'
            
            # é¦–å…ˆæ‰¾åˆ°æ‰€æœ‰URLçš„ä½ç½®ï¼Œä½œä¸ºåˆ‡åˆ†ç‚¹
            urls_in_text = list(re.finditer(url_pattern, content_text))
            
            if not urls_in_text:
                return []
            
            # æŒ‰ç…§URLä½ç½®åˆ‡åˆ†å†…å®¹
            for i, url_match in enumerate(urls_in_text):
                # ç¡®å®šå½“å‰æ–‡ç« çš„å¼€å§‹å’Œç»“æŸä½ç½®
                start_pos = 0 if i == 0 else urls_in_text[i-1].end()
                end_pos = urls_in_text[i+1].start() if i+1 < len(urls_in_text) else len(content_text)
                
                # æå–å½“å‰æ–‡ç« çš„å®Œæ•´æ–‡æœ¬å—
                article_block = content_text[start_pos:end_pos].strip()
                
                if not article_block:
                    continue
                
                # åˆ†æè¿™ä¸ªæ–‡æœ¬å—
                url = url_match.group()
                lines = article_block.split('\n')
                
                title = ''
                author = ''
                content_lines = []
                
                url_line_index = -1
                for j, line in enumerate(lines):
                    if url in line:
                        url_line_index = j
                        # URLå‰é¢çš„æ–‡æœ¬å¯èƒ½æ˜¯æ ‡é¢˜
                        url_pos = line.find(url)
                        if url_pos > 0:
                            title = line[:url_pos].strip()
                        break
                
                # å¦‚æœURLè¡Œå‰é¢è¿˜æœ‰è¡Œï¼Œå¯èƒ½æ˜¯æ ‡é¢˜
                if url_line_index > 0 and not title:
                    for j in range(url_line_index-1, -1, -1):
                        line = lines[j].strip()
                        if line and len(line) < 200:  # æ ‡é¢˜ä¸åº”è¯¥å¤ªé•¿
                            title = line
                            break
                
                # URLè¡Œåé¢çš„å†…å®¹
                for j in range(url_line_index + 1, len(lines)):
                    line = lines[j].strip()
                    if not line:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ä½œè€…æ ¼å¼ (Author:)
                    if ':' in line and len(line.split(':', 1)[0].strip()) < 20:
                        parts = line.split(':', 1)
                        potential_author = parts[0].strip()
                        remaining_content = parts[1].strip()
                        
                        # å¦‚æœçœ‹èµ·æ¥åƒä½œè€…å
                        if not re.search(r'https?://', potential_author):
                            author = potential_author
                            if remaining_content:
                                content_lines.append(remaining_content)
                        else:
                            content_lines.append(line)
                    else:
                        content_lines.append(line)
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„æ ‡é¢˜ï¼Œå°è¯•ä»å†…å®¹çš„ç¬¬ä¸€è¡Œæå–
                if not title and content_lines:
                    first_line = content_lines[0]
                    if len(first_line) < 150:  # å¯èƒ½æ˜¯æ ‡é¢˜
                        title = first_line
                        content_lines = content_lines[1:]
                
                content = '\n'.join(content_lines).strip()
                
                if title and url:
                    items.append({
                        'title': title,
                        'url': url,
                        'content': content,
                        'author': author
                    })
            
            return items
        
        def fix_url(url_str):
            """ä¿®å¤å¸¸è§çš„URLæ ¼å¼é—®é¢˜"""
            if not url_str:
                return ''
            
            url_str = url_str.strip()
            
            # å¤„ç†åŒ…å«å¤šä¸ªURLçš„æƒ…å†µï¼Œå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„
            if ' ' in url_str and ('http://' in url_str or 'https://' in url_str):
                parts = url_str.split()
                for part in parts:
                    if part.startswith(('http://', 'https://')):
                        url_str = part
                        break
            
            # å¤„ç†ä¸­æ–‡å‰ç¼€çš„URLï¼ˆå¦‚ï¼šä¸­æ–‡ç‰ˆï¼šhttp://...ï¼‰
            if 'ï¼šhttp' in url_str:
                url_str = url_str.split('ï¼šhttp')[1]
                url_str = 'http' + url_str
            elif ':http' in url_str:
                url_str = url_str.split(':http')[1]  
                url_str = 'http' + url_str
            
            # ä¿®å¤å¸¸è§çš„æ‹¼å†™é”™è¯¯
            url_str = url_str.replace('hu.baittps://', 'https://')
            url_str = url_str.replace('htps://', 'https://')
            url_str = url_str.replace('htp://', 'http://')
            
            # å¦‚æœæ²¡æœ‰åè®®å‰ç¼€ï¼Œæ·»åŠ https://
            if url_str and not url_str.startswith(('http://', 'https://', 'ftp://')):
                # æ£€æŸ¥æ˜¯å¦æ˜¯åŸŸåæ ¼å¼
                if '.' in url_str and not url_str.startswith('/'):
                    url_str = 'https://' + url_str
            
            # ç§»é™¤URLæœ«å°¾çš„å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
            url_str = url_str.rstrip()
            
            # æ£€æŸ¥URLæ˜¯å¦åŒ…å«æ— æ•ˆå­—ç¬¦ï¼ˆç©ºæ ¼ã€ä¸­æ–‡ç­‰ï¼‰
            if url_str.startswith(('http://', 'https://')):
                # å¦‚æœåŒ…å«ç©ºæ ¼æˆ–ä¸­æ–‡å­—ç¬¦ï¼Œå¯èƒ½æ˜¯æ— æ•ˆURL
                import re
                if re.search(r'[\s\u4e00-\u9fff]', url_str):
                    # å°è¯•æå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„URLéƒ¨åˆ†
                    match = re.search(r'https?://[^\s\u4e00-\u9fff]+', url_str)
                    if match:
                        url_str = match.group(0)
                    else:
                        return ''  # æ— æ³•ä¿®å¤ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
            
            return url_str
        
        def is_valid_article(title, url, content):
            """éªŒè¯æ–‡ç« æ•°æ®æ˜¯å¦æœ‰æ•ˆ"""
            if not title or not url:
                return False
            
            if len(title.strip()) < 2:
                return False
                
            if not url.startswith(('http://', 'https://')):
                return False
                
            return True
        
        def process_articles(articles, source_name):
            """å¤„ç†æ–‡ç« åˆ—è¡¨ï¼Œåº”ç”¨æ™ºèƒ½æ‹†åˆ†å’Œè¿‡æ»¤"""
            nonlocal split_count, filtered_count, time_filled_count
            processed_articles = []
            
            for article in articles:
                try:
                    if 'attributes' in article:
                        # ç»“æ„åŒ–æ ¼å¼ï¼š{id: x, attributes: {...}}
                        attrs = article['attributes']
                        title = safe_strip(attrs.get('title', ''))
                        url = safe_strip(attrs.get('url', ''))
                        author = safe_strip(attrs.get('author', ''))
                        introduce = safe_strip(attrs.get('introduce', ''))
                        full_content = safe_strip(attrs.get('full_content', ''))
                        episode = safe_strip(attrs.get('episode', ''))
                        time_str = safe_strip(attrs.get('time', ''))
                    else:
                        # ç®€å•æ ¼å¼ï¼šç›´æ¥æ˜¯æ–‡ç« å­—æ®µ
                        title = safe_strip(article.get('title', ''))
                        url = safe_strip(article.get('url', ''))
                        author = safe_strip(article.get('author', ''))
                        introduce = safe_strip(article.get('introduce', article.get('content', '')))
                        full_content = safe_strip(article.get('full_content', ''))
                        episode = safe_strip(article.get('episode', ''))
                        time_str = safe_strip(article.get('time', article.get('publish_time', '')))
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯åˆé›†æ–‡ç« ï¼ˆåŒ…å«å¤šä¸ªå­æ–‡ç« ï¼‰
                    content_to_parse = full_content or introduce
                    parsed_items = []
                    
                    if smart_split and content_to_parse and (title == "Web3 æå®¢æ—¥æŠ¥" or "æå®¢æ—¥æŠ¥" in title):
                        # è¿™æ˜¯ä¸€ä¸ªåˆé›†æ–‡ç« ï¼Œéœ€è¦æ‹†åˆ†
                        parsed_items = parse_content_items(content_to_parse)
                        if parsed_items:
                            split_count += len(parsed_items)
                            print(f"ğŸ“° {source_name}: æ‹†åˆ†åˆé›†æ–‡ç«  '{title}' â†’ {len(parsed_items)} ä¸ªå­æ–‡ç« ")
                    
                    # å¦‚æœæ²¡æœ‰æ‹†åˆ†å‡ºå­æ–‡ç« ï¼Œæˆ–è€…ä¸æ˜¯åˆé›†ï¼Œä¿æŒåŸæ ·
                    if not parsed_items:
                        parsed_items = [{
                            'title': title,
                            'url': url,
                            'content': content_to_parse,
                            'author': author
                        }]
                    
                    # å¤„ç†æ¯ä¸ªå­æ–‡ç« 
                    for parsed_item in parsed_items:
                        sub_title = parsed_item.get('title', '').strip()
                        sub_url = parsed_item.get('url', '').strip()
                        sub_content = parsed_item.get('content', '').strip()
                        sub_author = parsed_item.get('author', '').strip() or author
                        
                        # ä¿®å¤URL
                        sub_url = fix_url(sub_url)
                        
                        # è¿‡æ»¤åæ•°æ®
                        if filter_bad_data and not is_valid_article(sub_title, sub_url, sub_content):
                            filtered_count += 1
                            continue
                        
                        # å¤„ç†æ—¶é—´å­—æ®µ
                        final_time = time_str
                        if fill_time and not time_str:
                            final_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                            time_filled_count += 1
                        
                        # åˆ›å»ºå¤„ç†åçš„æ–‡ç« 
                        processed_article = {
                            'id': article.get('id', 0),
                            'attributes': {
                                'title': sub_title,
                                'url': sub_url,
                                'author': sub_author,
                                'introduce': sub_content[:200] + '...' if len(sub_content) > 200 else sub_content,
                                'full_content': sub_content,
                                'episode': episode,
                                'time': final_time
                            }
                        }
                        
                        processed_articles.append(processed_article)
                        
                except Exception as e:
                    print(f"âš ï¸  å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            return processed_articles
        
        # å¤„ç†ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„æ•°æ®
        print(f"ğŸ”„ å¤„ç†æ–‡ä»¶1æ•°æ®...")
        processed_articles1 = process_articles(articles1, "æ–‡ä»¶1")
        
        # å¤„ç†ç¬¬äºŒä¸ªæ–‡ä»¶çš„æ•°æ®  
        print(f"ğŸ”„ å¤„ç†æ–‡ä»¶2æ•°æ®...")
        processed_articles2 = process_articles(articles2, "æ–‡ä»¶2")
        
        # åˆå¹¶å¤„ç†åçš„æ•°æ®å¹¶å»é‡
        for article in processed_articles1 + processed_articles2:
            key = _get_dedup_key(article, dedup_strategy)
            if key and key not in seen:
                seen.add(key)
                merged_articles.append(article)
            elif key:
                duplicates += 1
        
        # æŒ‰IDæ’åº
        try:
            merged_articles.sort(key=lambda x: int(x.get('id', 0)), reverse=True)
        except:
            # å¦‚æœIDä¸æ˜¯æ•°å­—ï¼Œä½¿ç”¨åŸå§‹é¡ºåº
            pass
        
        # é‡æ–°ç¼–å·ID
        for idx, article in enumerate(merged_articles, 1):
            article['id'] = idx
        
        # è®¾ç½®è¾“å‡ºæ–‡ä»¶å
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"data/merged_articles_{timestamp}.json"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_file)
        if output_dir:  # åªæœ‰å½“æœ‰ç›®å½•è·¯å¾„æ—¶æ‰åˆ›å»º
            os.makedirs(output_dir, exist_ok=True)
        
        # åˆå¹¶å…ƒæ•°æ®
        merged_data = {
            "export_info": {
                "source": "MERGED",
                "source_files": [file1, file2],
                "total": len(merged_articles),
                "duplicates_removed": duplicates,
                "dedup_strategy": dedup_strategy,
                "merge_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "original_totals": {
                    "file1": len(articles1),
                    "file2": len(articles2)
                },
                "processing_stats": {
                    "smart_split_enabled": smart_split,
                    "filter_bad_data_enabled": filter_bad_data,
                    "fill_time_enabled": fill_time,
                    "articles_split": split_count,
                    "bad_data_filtered": filtered_count,
                    "time_fields_filled": time_filled_count
                }
            },
            "articles": merged_articles
        }
        
        # ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… åˆå¹¶å®Œæˆï¼")
        print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"ğŸ“Š åˆå¹¶ç»“æœ:")
        print(f"  - åŸå§‹æ€»æ•°: {len(articles1) + len(articles2)}")
        print(f"  - å¤„ç†åæ€»æ•°: {len(processed_articles1) + len(processed_articles2)}")
        print(f"  - å»é‡åæ€»æ•°: {len(merged_articles)}")
        print(f"  - åˆ é™¤é‡å¤: {duplicates}")
        print(f"  - å»é‡ç­–ç•¥: {dedup_strategy}")
        
        # æ™ºèƒ½å¤„ç†ç»Ÿè®¡
        if smart_split or filter_bad_data or fill_time:
            print(f"\nğŸ§  æ™ºèƒ½å¤„ç†ç»Ÿè®¡:")
            if smart_split and split_count > 0:
                print(f"  - æ‹†åˆ†å­æ–‡ç« : {split_count} ä¸ª")
            if filter_bad_data and filtered_count > 0:
                print(f"  - è¿‡æ»¤åæ•°æ®: {filtered_count} ä¸ª")
            if fill_time and time_filled_count > 0:
                print(f"  - è¡¥å…¨æ—¶é—´: {time_filled_count} ä¸ª")
        
        # æ•°æ®ç»Ÿè®¡
        print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
        
        # ç»Ÿè®¡ä½œè€…åˆ†å¸ƒ
        authors = {}
        for article in merged_articles:
            author = article.get('attributes', {}).get('author', 'Unknown')
            authors[author] = authors.get(author, 0) + 1
        
        print(f"  ä½œè€…æ•°é‡: {len(authors)}")
        print(f"  çƒ­é—¨ä½œè€…:")
        for author, count in sorted(authors.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"    {author}: {count} ç¯‡")
        
        # ç»Ÿè®¡æ—¶é—´åˆ†å¸ƒ
        years = {}
        for article in merged_articles:
            time_str = article.get('attributes', {}).get('time', '')
            if time_str:
                try:
                    year = time_str.split('-')[0]
                    years[year] = years.get(year, 0) + 1
                except:
                    pass
        
        if years:
            print(f"  æ—¶é—´åˆ†å¸ƒ:")
            for year, count in sorted(years.items(), reverse=True)[:5]:
                print(f"    {year}: {count} ç¯‡")
        
        # ç»Ÿè®¡æœŸæ•°åˆ†å¸ƒï¼ˆå¦‚æœæœ‰ï¼‰
        episodes = {}
        for article in merged_articles:
            episode = article.get('attributes', {}).get('episode', '')
            if episode:
                episodes[episode] = episodes.get(episode, 0) + 1
        
        if episodes:
            print(f"  æœŸæ•°ç»Ÿè®¡: {len(episodes)} ä¸ªä¸åŒæœŸæ•°")
            print(f"  æœ€æ–°æœŸæ•°: {max(episodes.keys()) if episodes else 'None'}")
        
        return output_file
        
    except FileNotFoundError as e:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {str(e)}")
        raise
    except json.JSONDecodeError as e:
        print(f"âŒ JSONæ ¼å¼é”™è¯¯: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ åˆå¹¶å¤±è´¥: {str(e)}")
        raise


def _get_dedup_key(article: dict, strategy: str) -> str:
    """æ ¹æ®ç­–ç•¥è·å–å»é‡é”®"""
    
    attributes = article.get('attributes', {})
    
    if strategy == 'url':
        # ä½¿ç”¨URLä½œä¸ºå»é‡é”®
        url = attributes.get('url', '')
        if url:
            return url.strip()
    
    elif strategy == 'title':
        # ä½¿ç”¨æ ‡é¢˜ä½œä¸ºå»é‡é”®
        title = attributes.get('title', '')
        if title:
            return title.strip()
    
    elif strategy == 'episode':
        # ä½¿ç”¨æœŸæ•°ä½œä¸ºå»é‡é”®
        episode = attributes.get('episode', '')
        if episode:
            return episode.strip()
    
    elif strategy == 'title_author':
        # ä½¿ç”¨æ ‡é¢˜+ä½œè€…ä½œä¸ºå»é‡é”®
        title = attributes.get('title', '').strip()
        author = attributes.get('author', '').strip()
        if title and author:
            return f"{title}_{author}"
    
    elif strategy == 'url_title':
        # ä½¿ç”¨URL+æ ‡é¢˜ä½œä¸ºå»é‡é”®
        url = attributes.get('url', '').strip()
        title = attributes.get('title', '').strip()
        if url and title:
            return f"{url}_{title}"
    
    # é»˜è®¤ä½¿ç”¨URLï¼Œå¦‚æœæ²¡æœ‰URLåˆ™ä½¿ç”¨æ ‡é¢˜
    url = attributes.get('url', '').strip()
    if url:
        return url
    
    title = attributes.get('title', '').strip()
    if title:
        return title
    
    # å¦‚æœéƒ½æ²¡æœ‰ï¼Œè¿”å›Noneï¼ˆä¸å»é‡ï¼‰
    return None


async def analyze_json_files(*files: str):
    """åˆ†æJSONæ–‡ä»¶çš„å†…å®¹å’Œç»“æ„"""
    
    print(f"ğŸ” åˆ†æ {len(files)} ä¸ªJSONæ–‡ä»¶")
    print("=" * 60)
    
    for i, file_path in enumerate(files, 1):
        try:
            print(f"\nğŸ“ æ–‡ä»¶ {i}: {file_path}")
            
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            articles = data.get('articles', [])
            export_info = data.get('export_info', {})
            
            print(f"ğŸ“Š åŸºæœ¬ä¿¡æ¯:")
            print(f"  - æ•°æ®æ€»æ•°: {len(articles)}")
            print(f"  - æ•°æ®æº: {export_info.get('source', 'Unknown')}")
            print(f"  - å¯¼å‡ºæ—¶é—´: {export_info.get('export_time', export_info.get('merge_time', 'Unknown'))}")
            
            if articles:
                # åˆ†ææ•°æ®ç»“æ„
                sample = articles[0]
                attributes = sample.get('attributes', {})
                
                print(f"ğŸ“‹ æ•°æ®ç»“æ„:")
                print(f"  - IDèŒƒå›´: {min(a.get('id', 0) for a in articles)} - {max(a.get('id', 0) for a in articles)}")
                print(f"  - å­—æ®µ: {list(attributes.keys())}")
                
                # ç»Ÿè®¡æœ‰æ•ˆæ•°æ®
                has_url = sum(1 for a in articles if a.get('attributes', {}).get('url'))
                has_title = sum(1 for a in articles if a.get('attributes', {}).get('title'))
                has_episode = sum(1 for a in articles if a.get('attributes', {}).get('episode'))
                
                print(f"ğŸ“ˆ æ•°æ®å®Œæ•´æ€§:")
                print(f"  - æœ‰URL: {has_url}/{len(articles)} ({has_url/len(articles)*100:.1f}%)")
                print(f"  - æœ‰æ ‡é¢˜: {has_title}/{len(articles)} ({has_title/len(articles)*100:.1f}%)")
                print(f"  - æœ‰æœŸæ•°: {has_episode}/{len(articles)} ({has_episode/len(articles)*100:.1f}%)")
                
                # é¢„è§ˆæ•°æ®
                print(f"ğŸ” æ•°æ®é¢„è§ˆ:")
                for j, article in enumerate(articles[:3], 1):
                    attrs = article.get('attributes', {})
                    print(f"  {j}. {attrs.get('episode', 'N/A')} - {attrs.get('title', 'No Title')[:40]}...")
                    print(f"     ä½œè€…: {attrs.get('author', 'Unknown')} | æ—¶é—´: {attrs.get('time', 'N/A')}")
            
        except Exception as e:
            print(f"âŒ åˆ†ææ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
    
    print(f"\nğŸ’¡ åˆå¹¶å»ºè®®:")
    print(f"  - ä½¿ç”¨URLå»é‡: python main.py merge file1.json file2.json --strategy url")
    print(f"  - ä½¿ç”¨æ ‡é¢˜å»é‡: python main.py merge file1.json file2.json --strategy title")
    print(f"  - ä½¿ç”¨æœŸæ•°å»é‡: python main.py merge file1.json file2.json --strategy episode")


async def clean_json_data(input_file: str, output_file: str = None, 
                         require_fields: List[str] = None, 
                         clean_empty: bool = True,
                         validate_urls: bool = False,
                         exclude_fields: List[str] = None):
    """æ¸…æ´—JSONæ•°æ®ï¼Œåˆ é™¤ä¸å®Œæ•´çš„æ¡ç›®"""
    
    # è®¾ç½®é»˜è®¤çš„å¿…éœ€å­—æ®µ
    if require_fields is None:
        require_fields = ['author', 'url', 'title']
    
    # è®¾ç½®é»˜è®¤çš„æ’é™¤å­—æ®µ
    if exclude_fields is None:
        exclude_fields = []
    
    try:
        print(f"ğŸ§¹ å¼€å§‹æ¸…æ´—æ•°æ®: {input_file}")
        print(f"ğŸ“‹ æ¸…æ´—è§„åˆ™:")
        print(f"  - å¿…éœ€å­—æ®µ: {', '.join(require_fields)}")
        print(f"  - æ¸…ç†ç©ºå€¼: {clean_empty}")
        print(f"  - éªŒè¯URL: {validate_urls}")
        if exclude_fields:
            print(f"  - æ’é™¤å­—æ®µ: {', '.join(exclude_fields)}")
        
        # è¯»å–åŸå§‹æ–‡ä»¶
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        original_articles = data.get('articles', [])
        print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(original_articles)} æ¡")
        
        # æ¸…æ´—æ•°æ®
        cleaned_articles = []
        removed_count = 0
        removal_reasons = {
            'missing_required_fields': 0,
            'empty_values': 0,
            'invalid_urls': 0,
            'empty_title': 0,
            'empty_author': 0,
            'empty_url': 0
        }
        
        for article in original_articles:
            attributes = article.get('attributes', {})
            should_remove = False
            
            # æ£€æŸ¥å¿…éœ€å­—æ®µæ˜¯å¦å­˜åœ¨ä¸”éç©º
            for field in require_fields:
                field_value = attributes.get(field)
                if field_value is not None:
                    field_value = str(field_value).strip()
                else:
                    field_value = ''
                
                if not field_value:
                    should_remove = True
                    removal_reasons['missing_required_fields'] += 1
                    removal_reasons[f'empty_{field}'] += 1
                    break
            
            # å¦‚æœå¯ç”¨äº†æ¸…ç†ç©ºå€¼ï¼Œæ£€æŸ¥å…¶ä»–é‡è¦å­—æ®µ
            if not should_remove and clean_empty:
                # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦æœ‰æ„ä¹‰ï¼ˆä¸åªæ˜¯ç©ºæ ¼æˆ–ç‰¹æ®Šå­—ç¬¦ï¼‰
                title = attributes.get('title')
                if title is not None:
                    title = str(title).strip()
                    if title and len(title) < 2:  # æ ‡é¢˜å¤ªçŸ­
                        should_remove = True
                        removal_reasons['empty_values'] += 1
                
                # æ£€æŸ¥ä»‹ç»æ˜¯å¦è¿‡çŸ­
                introduce = attributes.get('introduce')
                if introduce is not None:
                    introduce = str(introduce).strip()
                    if introduce and len(introduce) < 5:  # ä»‹ç»å¤ªçŸ­
                        # ä¸åˆ é™¤ï¼Œä½†è®°å½•
                        pass
            
            # å¦‚æœå¯ç”¨äº†URLéªŒè¯
            if not should_remove and validate_urls:
                url = attributes.get('url')
                if url is not None:
                    url = str(url).strip()
                    if url and not _is_valid_url(url):
                        should_remove = True
                        removal_reasons['invalid_urls'] += 1
            
            # ä¿ç•™æœ‰æ•ˆçš„æ–‡ç« 
            if not should_remove:
                # å¦‚æœæœ‰å­—æ®µéœ€è¦æ’é™¤ï¼Œåˆ›å»ºæ¸…ç†åçš„æ–‡ç« å‰¯æœ¬
                if exclude_fields:
                    cleaned_article = article.copy()
                    cleaned_attributes = cleaned_article.get('attributes', {}).copy()
                    
                    # ç§»é™¤æ’é™¤çš„å­—æ®µ
                    for exclude_field in exclude_fields:
                        if exclude_field in cleaned_attributes:
                            del cleaned_attributes[exclude_field]
                    
                    cleaned_article['attributes'] = cleaned_attributes
                    cleaned_articles.append(cleaned_article)
                else:
                    cleaned_articles.append(article)
            else:
                removed_count += 1
        
        # é‡æ–°ç¼–å·ID
        for idx, article in enumerate(cleaned_articles, 1):
            article['id'] = idx
        
        # è®¾ç½®è¾“å‡ºæ–‡ä»¶å
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            base_name = os.path.splitext(os.path.basename(input_file))[0]
            output_file = f"data/{base_name}_cleaned_{timestamp}.json"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # æ›´æ–°å…ƒæ•°æ®
        cleaned_data = data.copy()
        original_export_info = data.get('export_info', {})
        
        cleaned_data['export_info'] = {
            **original_export_info,
            "source": f"CLEANED_{original_export_info.get('source', 'UNKNOWN')}",
            "original_file": input_file,
            "total": len(cleaned_articles),
            "original_total": len(original_articles),
            "removed_count": removed_count,
            "removal_reasons": removal_reasons,
            "clean_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "clean_rules": {
                "require_fields": require_fields,
                "clean_empty": clean_empty,
                "validate_urls": validate_urls,
                "exclude_fields": exclude_fields
            }
        }
        cleaned_data['articles'] = cleaned_articles
        
        # ä¿å­˜æ¸…æ´—åçš„æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(cleaned_data, f, ensure_ascii=False, indent=2)
        
        # æ˜¾ç¤ºæ¸…æ´—ç»“æœ
        print(f"\nâœ… æ•°æ®æ¸…æ´—å®Œæˆï¼")
        print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"ğŸ“Š æ¸…æ´—ç»“æœ:")
        print(f"  - åŸå§‹æ•°æ®: {len(original_articles)} æ¡")
        print(f"  - æ¸…æ´—åæ•°æ®: {len(cleaned_articles)} æ¡")
        print(f"  - åˆ é™¤æ•°æ®: {removed_count} æ¡")
        print(f"  - ä¿ç•™ç‡: {len(cleaned_articles)/len(original_articles)*100:.1f}%")
        
        # æ˜¾ç¤ºåˆ é™¤åŸå› ç»Ÿè®¡
        if removed_count > 0:
            print(f"\nğŸ—‘ï¸ åˆ é™¤åŸå› ç»Ÿè®¡:")
            for reason, count in removal_reasons.items():
                if count > 0:
                    print(f"  - {reason}: {count} æ¡")
        
        # æ•°æ®è´¨é‡åˆ†æ
        print(f"\nğŸ“ˆ æ•°æ®è´¨é‡åˆ†æ:")
        
        # ç»Ÿè®¡å­—æ®µå®Œæ•´æ€§
        field_completeness = {}
        for field in ['author', 'url', 'title', 'episode', 'time', 'introduce']:
            complete_count = 0
            for a in cleaned_articles:
                field_value = a.get('attributes', {}).get(field)
                if field_value is not None and str(field_value).strip():
                    complete_count += 1
            field_completeness[field] = complete_count / len(cleaned_articles) * 100 if cleaned_articles else 0
        
        for field, percentage in field_completeness.items():
            print(f"  - {field}: {percentage:.1f}% å®Œæ•´")
        
        return output_file
        
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {input_file}")
        raise
    except json.JSONDecodeError as e:
        print(f"âŒ JSONæ ¼å¼é”™è¯¯: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ æ¸…æ´—å¤±è´¥: {str(e)}")
        raise


def _is_valid_url(url: str) -> bool:
    """éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ"""
    import re
    
    # åŸºæœ¬URLæ ¼å¼éªŒè¯
    url_pattern = re.compile(
        r'^https?://'  # http:// or https://
        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain...
        r'localhost|'  # localhost...
        r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
        r'(?::\d+)?'  # optional port
        r'(?:/?|[/?]\S+)$', re.IGNORECASE)
    
    return url_pattern.match(url) is not None


async def batch_clean_directory(directory: str, pattern: str = "*.json", 
                               require_fields: List[str] = None,
                               clean_empty: bool = True,
                               validate_urls: bool = False,
                               exclude_fields: List[str] = None):
    """æ‰¹é‡æ¸…æ´—ç›®å½•ä¸­çš„JSONæ–‡ä»¶"""
    import glob
    
    print(f"ğŸ”„ æ‰¹é‡æ¸…æ´—ç›®å½•: {directory}")
    print(f"ğŸ“ æ–‡ä»¶æ¨¡å¼: {pattern}")
    
    # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
    search_pattern = os.path.join(directory, pattern)
    files = glob.glob(search_pattern)
    
    if not files:
        print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶: {search_pattern}")
        return
    
    print(f"ğŸ“‹ æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶å¾…å¤„ç†")
    
    results = []
    
    for file_path in files:
        try:
            print(f"\nğŸ§¹ å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")
            result = await clean_json_data(
                file_path, 
                require_fields=require_fields,
                clean_empty=clean_empty,
                validate_urls=validate_urls,
                exclude_fields=exclude_fields
            )
            results.append(result)
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
            continue
    
    print(f"\nâœ… æ‰¹é‡æ¸…æ´—å®Œæˆï¼")
    print(f"ğŸ“Š å¤„ç†ç»“æœ: {len(results)}/{len(files)} ä¸ªæ–‡ä»¶æˆåŠŸ")
    
    if results:
        print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶:")
        for result in results:
            print(f"  - {result}")
    
    return results


async def crawl_account(account_name: str, max_articles: Optional[int] = None, use_proxy: bool = False):
    """
    Crawl articles from a WeChat public account
    
    Args:
        account_name: Name of the WeChat public account
        max_articles: Maximum number of articles to crawl
        use_proxy: Whether to use proxy rotation
    """
    # Initialize components
    db = DatabaseManager(use_mongodb=False)  # Use SQLite for simplicity
    proxy_manager = ProxyManager() if use_proxy else None
    
    # Create crawl job
    job = CrawlJob(
        account_name=account_name,
        status="running",
        start_time=datetime.now()
    )
    job_id = db.create_job(job)
    logger.info(f"Created crawl job {job_id} for account '{account_name}'")
    
    try:
        # Initialize proxy manager if needed
        if proxy_manager:
            await proxy_manager.initialize()
        
        # Create crawler
        async with WeChatCrawler() as crawler:
            # Set proxy if available
            if proxy_manager:
                proxy = proxy_manager.get_next_proxy()
                if proxy:
                    await crawler.browser_manager.stop()
                    await crawler.browser_manager.start(proxy=proxy.to_playwright_proxy())
                    logger.info(f"Using proxy: {proxy.host}:{proxy.port}")
            
            # Crawl articles
            logger.info(f"Starting to crawl account '{account_name}'...")
            articles = await crawler.crawl_account(account_name, max_articles)
            
            # Save articles to database
            saved_count = 0
            for article in articles:
                if db.save_article(article):
                    saved_count += 1
            
            # Update job status
            db.update_job(job_id, {
                "status": "completed",
                "total_articles": len(articles),
                "crawled_articles": saved_count,
                "failed_articles": len(articles) - saved_count,
                "end_time": datetime.now()
            })
            
            logger.info(f"Crawl completed! Found {len(articles)} articles, saved {saved_count} new articles")
            
    except Exception as e:
        logger.error(f"Crawl failed: {str(e)}")
        db.update_job(job_id, {
            "status": "failed",
            "error_message": str(e),
            "end_time": datetime.now()
        })
        raise
    finally:
        db.close()


async def crawl_single_article(article_url: str, show_content: bool = False):
    """
    Crawl a single WeChat article
    
    Args:
        article_url: URL of the article to crawl
        show_content: Whether to display article content in console
    """
    db = DatabaseManager(use_mongodb=False)
    
    try:
        async with WeChatCrawler() as crawler:
            logger.info(f"Crawling article: {article_url}")
            article = await crawler.crawl_article(article_url)
            
            if article:
                # Display article info
                print("\n" + "="*60)
                print(f"ğŸ“„ Title: {article.title}")
                print(f"âœï¸  Author: {article.author or 'Unknown'}")
                print(f"ğŸ“± Account: {article.account_name or 'Unknown'}")
                print(f"ğŸ“… Publish Time: {article.publish_time or 'Unknown'}")
                print(f"ğŸ–¼ï¸  Images: {len(article.images)} found")
                print(f"ğŸ“ Content length: {len(article.content)} characters")
                
                if show_content and article.content:
                    print("\nğŸ“– Article Content:")
                    print("="*60)
                    paragraphs = article.content.split('\n')
                    paragraphs = [p.strip() for p in paragraphs if p.strip()]
                    
                    for i, paragraph in enumerate(paragraphs[:10]):  # Show first 10 paragraphs
                        if len(paragraph) > 300:
                            paragraph = paragraph[:300] + "..."
                        print(f"\n{i+1}. {paragraph}")
                    
                    if len(paragraphs) > 10:
                        print(f"\n... and {len(paragraphs) - 10} more paragraphs")
                
                # Save to database
                if db.save_article(article):
                    print("âœ… Successfully saved article to database")
                else:
                    print("â„¹ï¸  Article already exists in database")
                    
            else:
                print("âŒ Failed to crawl article")
                
    except Exception as e:
        logger.error(f"Error crawling article: {str(e)}")
        raise
    finally:
        db.close()


async def test_crawler(show_content: bool = False):
    """Test crawler with predefined article"""
    test_url = "https://mp.weixin.qq.com/s/7gWx7Lj8AOOKnD7KscgNMg"
    print(f"ğŸ§ª Testing crawler with: {test_url}")
    await crawl_single_article(test_url, show_content)


async def get_history_articles(article_url: str, max_articles: Optional[int] = None, use_proxy: bool = False):
    """
    ä»å•ç¯‡æ–‡ç« è·å–è¯¥å…¬ä¼—å·çš„å†å²æ–‡ç« 
    
    Args:
        article_url: èµ·å§‹æ–‡ç« URL
        max_articles: æœ€å¤§è·å–æ•°é‡
        use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
    """
    # Initialize components
    db = DatabaseManager(use_mongodb=False)
    proxy_manager = ProxyManager() if use_proxy else None
    
    try:
        # Initialize proxy manager if needed
        if proxy_manager:
            await proxy_manager.initialize()
        
        # è·å–å†å²æ–‡ç« åˆ—è¡¨
        async with HistoryCrawler() as crawler:
            # Set proxy if available
            if proxy_manager:
                proxy = proxy_manager.get_next_proxy()
                if proxy:
                    await crawler.browser_manager.stop()
                    await crawler.browser_manager.start(proxy=proxy.to_playwright_proxy())
                    logger.info(f"Using proxy: {proxy.host}:{proxy.port}")
            
            logger.info(f"å¼€å§‹è·å–å†å²æ–‡ç« : {article_url}")
            history_articles = await crawler.get_history_articles(article_url, max_articles)
            account_info = crawler.get_account_info()
            account_name = account_info.get('name', 'Unknown Account')
            
            logger.info(f"å‘ç° {len(history_articles)} ç¯‡å†å²æ–‡ç« ï¼Œè´¦å·: {account_name}")
        
        if not history_articles:
            logger.warning("æœªå‘ç°å†å²æ–‡ç« ")
            print("âŒ æœªæ‰¾åˆ°å†å²æ–‡ç« ï¼Œå¯èƒ½çš„åŸå› ï¼š")
            print("   - è¯¥å…¬ä¼—å·æ²¡æœ‰å¼€æ”¾å†å²æ–‡ç« ")
            print("   - éœ€è¦å…³æ³¨åæ‰èƒ½æŸ¥çœ‹å†å²æ–‡ç« ")
            print("   - åçˆ¬è™«æœºåˆ¶é˜»æ­¢äº†è®¿é—®")
            return
        
        # æ˜¾ç¤ºå‘ç°çš„æ–‡ç« åˆ—è¡¨
        print(f"\nğŸ“š å‘ç° {len(history_articles)} ç¯‡å†å²æ–‡ç« ")
        print("=" * 80)
        
        for i, article in enumerate(history_articles[:10]):  # æ˜¾ç¤ºå‰10ç¯‡
            print(f"{i+1:2d}. {article['title']}")
            print(f"    URL: {article['url']}")
            print(f"    æ¥æº: {article['source']}")
            print()
        
        if len(history_articles) > 10:
            print(f"... è¿˜æœ‰ {len(history_articles) - 10} ç¯‡æ–‡ç« ")
        
        # è¯¢é—®æ˜¯å¦å¼€å§‹çˆ¬å–
        print(f"\nğŸ¤” æ˜¯å¦å¼€å§‹çˆ¬å–è¿™äº›æ–‡ç« ï¼Ÿ(y/N): ", end="")
        try:
            import sys
            response = input().lower().strip()
            if response in ['y', 'yes', 'æ˜¯']:
                # å¼€å§‹æ‰¹é‡çˆ¬å–
                await _batch_crawl_articles(history_articles, account_name, db, proxy_manager)
            else:
                print("å–æ¶ˆçˆ¬å–")
        except KeyboardInterrupt:
            print("\nå–æ¶ˆçˆ¬å–")
        
    except Exception as e:
        logger.error(f"è·å–å†å²æ–‡ç« å¤±è´¥: {str(e)}")
        raise
    finally:
        db.close()


async def _batch_crawl_articles(articles: list, account_name: str, db, proxy_manager):
    """æ‰¹é‡çˆ¬å–æ–‡ç« """
    # Create crawl job
    job = CrawlJob(
        account_name=account_name,
        status="running",
        start_time=datetime.now(),
        total_articles=len(articles)
    )
    job_id = db.create_job(job)
    logger.info(f"åˆ›å»ºçˆ¬å–ä»»åŠ¡ {job_id}ï¼Œå…± {len(articles)} ç¯‡æ–‡ç« ")
    
    try:
        async with WeChatCrawler() as crawler:
            # Set proxy if available
            if proxy_manager:
                proxy = proxy_manager.get_next_proxy()
                if proxy:
                    await crawler.browser_manager.stop()
                    await crawler.browser_manager.start(proxy=proxy.to_playwright_proxy())
            
            saved_count = 0
            failed_count = 0
            
            for i, article_info in enumerate(articles):
                try:
                    url = article_info['url']
                    title = article_info['title']
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å¼‚å¸¸URL
                    if _is_error_url(url) or _is_error_title(title):
                        failed_count += 1
                        logger.warning(f"è·³è¿‡å¼‚å¸¸æ–‡ç«  {i+1}/{len(articles)}: {title} - {url}")
                        print(f"âš ï¸  è·³è¿‡å¼‚å¸¸: {title}")
                        continue
                    
                    logger.info(f"çˆ¬å–æ–‡ç«  {i+1}/{len(articles)}: {title}")
                    
                    # å¢åŠ éšæœºå»¶è¿Ÿä»¥é¿å…æ£€æµ‹
                    import random
                    delay = random.uniform(5, 15)  # 5-15ç§’éšæœºå»¶è¿Ÿ
                    logger.info(f"ç­‰å¾… {delay:.1f} ç§’ä»¥é¿å…åçˆ¬æ£€æµ‹...")
                    await asyncio.sleep(delay)
                    
                    article = await crawler.crawl_article(url)
                    
                    if article:
                        if db.save_article(article):
                            saved_count += 1
                            print(f"âœ… å·²ä¿å­˜: {article.title}")
                        else:
                            print(f"â„¹ï¸  å·²å­˜åœ¨: {article.title}")
                    else:
                        failed_count += 1
                        print(f"âŒ å¤±è´¥: {article_info['title']}")
                    
                    # Update job progress
                    db.update_job(job_id, {
                        "crawled_articles": saved_count,
                        "failed_articles": failed_count
                    })
                    
                    # Delay between requests
                    await asyncio.sleep(settings.crawl_delay)
                    
                except Exception as e:
                    failed_count += 1
                    logger.error(f"çˆ¬å–æ–‡ç« å¤±è´¥ {article_info.get('url', '')}: {str(e)}")
                    continue
            
            # Update final job status
            db.update_job(job_id, {
                "status": "completed",
                "crawled_articles": saved_count,
                "failed_articles": failed_count,
                "end_time": datetime.now()
            })
            
            print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼ä¿å­˜ {saved_count} ç¯‡æ–°æ–‡ç« ï¼Œ{failed_count} ç¯‡å¤±è´¥")
            
    except Exception as e:
        logger.error(f"æ‰¹é‡çˆ¬å–å¤±è´¥: {str(e)}")
        db.update_job(job_id, {
            "status": "failed",
            "error_message": str(e),
            "end_time": datetime.now()
        })


async def get_series_articles(article_url: str, max_articles: Optional[int] = None, use_proxy: bool = False):
    """
    ä»ç³»åˆ—/åˆé›†æ–‡ç« è·å–æ‰€æœ‰ç›¸å…³æ–‡ç« 
    
    Args:
        article_url: ç³»åˆ—æ–‡ç« URL
        max_articles: æœ€å¤§è·å–æ•°é‡
        use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
    """
    # Initialize components
    db = DatabaseManager(use_mongodb=False)
    proxy_manager = ProxyManager() if use_proxy else None
    
    try:
        # Initialize proxy manager if needed
        if proxy_manager:
            await proxy_manager.initialize()
        
        # è·å–ç³»åˆ—æ–‡ç« åˆ—è¡¨
        async with SeriesCrawler() as crawler:
            # Set proxy if available
            if proxy_manager:
                proxy = proxy_manager.get_next_proxy()
                if proxy:
                    await crawler.browser_manager.stop()
                    await crawler.browser_manager.start(proxy=proxy.to_playwright_proxy())
                    logger.info(f"Using proxy: {proxy.host}:{proxy.port}")
            
            logger.info(f"å¼€å§‹è·å–ç³»åˆ—æ–‡ç« : {article_url}")
            series_articles = await crawler.get_series_articles(article_url, max_articles)
            series_info = crawler.get_series_info()
            series_name = series_info.get('album_name', 'Unknown Series')
            
            logger.info(f"å‘ç° {len(series_articles)} ç¯‡ç³»åˆ—æ–‡ç« ï¼Œç³»åˆ—: {series_name}")
        
        if not series_articles:
            logger.warning("æœªå‘ç°ç³»åˆ—æ–‡ç« ")
            print("âŒ æœªæ‰¾åˆ°ç³»åˆ—æ–‡ç« ï¼Œå¯èƒ½çš„åŸå› ï¼š")
            print("   - è¯¥æ–‡ç« ä¸å±äºä»»ä½•åˆé›†/ç³»åˆ—")
            print("   - åˆé›†ç›®å½•è®¿é—®å—é™")
            print("   - æ–‡ç« æ ‡é¢˜ä¸ç¬¦åˆç³»åˆ—æ¨¡å¼")
            return
        
        # æ˜¾ç¤ºå‘ç°çš„ç³»åˆ—æ–‡ç« åˆ—è¡¨
        print(f"\nğŸ“– å‘ç°ç³»åˆ—æ–‡ç« ï¼š{series_name}")
        print(f"ğŸ“Š å…± {len(series_articles)} ç¯‡æ–‡ç« ")
        print("=" * 80)
        
        for i, article in enumerate(series_articles):
            print(f"{i+1:3d}. {article['title']}")
            print(f"     æ¥æº: {article['source']} | URL: {article['url'][:50]}...")
            print()
        
        # è¯¢é—®æ˜¯å¦å¼€å§‹çˆ¬å–
        print(f"\nğŸ¤” æ˜¯å¦å¼€å§‹çˆ¬å–è¿™ {len(series_articles)} ç¯‡ç³»åˆ—æ–‡ç« ï¼Ÿ(y/N): ", end="")
        try:
            response = input().lower().strip()
            if response in ['y', 'yes', 'æ˜¯']:
                # å¼€å§‹æ‰¹é‡çˆ¬å–
                await _batch_crawl_articles(series_articles, series_name, db, proxy_manager)
            else:
                print("å–æ¶ˆçˆ¬å–")
        except KeyboardInterrupt:
            print("\nå–æ¶ˆçˆ¬å–")
        
    except Exception as e:
        logger.error(f"è·å–ç³»åˆ—æ–‡ç« å¤±è´¥: {str(e)}")
        raise
    finally:
        db.close()


async def discover_from_article(article_url: str, max_articles: Optional[int] = None, use_proxy: bool = False):
    """
    Discover and crawl articles from a WeChat account starting from a single article
    
    Args:
        article_url: URL of a WeChat article to start from
        max_articles: Maximum number of articles to discover and crawl
        use_proxy: Whether to use proxy rotation
    """
    # Initialize components
    db = DatabaseManager(use_mongodb=False)
    proxy_manager = ProxyManager() if use_proxy else None
    
    try:
        # Initialize proxy manager if needed
        if proxy_manager:
            await proxy_manager.initialize()
        
        # Discover articles starting from the given URL
        async with ArticleDiscovery() as discovery:
            # Set proxy if available
            if proxy_manager:
                proxy = proxy_manager.get_next_proxy()
                if proxy:
                    await discovery.browser_manager.stop()
                    await discovery.browser_manager.start(proxy=proxy.to_playwright_proxy())
                    logger.info(f"Using proxy: {proxy.host}:{proxy.port}")
            
            logger.info(f"Starting article discovery from: {article_url}")
            discovered_urls = await discovery.discover_from_article(article_url, max_articles)
            account_name = discovery.get_account_name() or "Unknown Account"
            
            logger.info(f"Discovered {len(discovered_urls)} articles from account: {account_name}")
        
        if not discovered_urls:
            logger.warning("No articles discovered")
            return
        
        # Create crawl job
        job = CrawlJob(
            account_name=account_name,
            status="running",
            start_time=datetime.now(),
            total_articles=len(discovered_urls)
        )
        job_id = db.create_job(job)
        logger.info(f"Created crawl job {job_id} for {len(discovered_urls)} discovered articles")
        
        # Crawl all discovered articles
        async with WeChatCrawler() as crawler:
            # Set proxy if available
            if proxy_manager:
                proxy = proxy_manager.get_next_proxy()
                if proxy:
                    await crawler.browser_manager.stop()
                    await crawler.browser_manager.start(proxy=proxy.to_playwright_proxy())
            
            saved_count = 0
            failed_count = 0
            
            for i, url in enumerate(discovered_urls):
                try:
                    logger.info(f"Crawling article {i+1}/{len(discovered_urls)}: {url}")
                    article = await crawler.crawl_article(url)
                    
                    if article:
                        if db.save_article(article):
                            saved_count += 1
                            logger.info(f"Saved article: {article.title}")
                        else:
                            logger.info(f"Article already exists: {article.title}")
                    else:
                        failed_count += 1
                        logger.warning(f"Failed to parse article: {url}")
                    
                    # Update job progress
                    db.update_job(job_id, {
                        "crawled_articles": saved_count,
                        "failed_articles": failed_count
                    })
                    
                    # Delay between requests
                    await asyncio.sleep(settings.crawl_delay)
                    
                except Exception as e:
                    failed_count += 1
                    logger.error(f"Error crawling article {url}: {str(e)}")
                    continue
            
            # Update final job status
            db.update_job(job_id, {
                "status": "completed",
                "crawled_articles": saved_count,
                "failed_articles": failed_count,
                "end_time": datetime.now()
            })
            
            logger.info(f"Discovery crawl completed! Saved {saved_count} new articles, {failed_count} failed")
            
    except Exception as e:
        logger.error(f"Discovery crawl failed: {str(e)}")
        if 'job_id' in locals():
            db.update_job(job_id, {
                "status": "failed",
                "error_message": str(e),
                "end_time": datetime.now()
            })
        raise
    finally:
        db.close()


async def show_stats():
    """Show database statistics"""
    db = DatabaseManager(use_mongodb=False)
    
    try:
        total_articles = db.get_article_count()
        
        # Get articles by account
        all_articles = db.get_articles_by_account("", limit=1000)
        accounts = {}
        for article in all_articles:
            account_name = article.get('account_name', 'Unknown')
            if account_name not in accounts:
                accounts[account_name] = 0
            accounts[account_name] += 1
        
        # Get recent jobs
        recent_jobs = db.get_jobs(limit=10)
        
        print("\n=== WeChat Crawler Statistics ===")
        print(f"ğŸ“Š Total Articles: {total_articles}")
        print(f"ğŸ‘¥ Total Accounts: {len(accounts)}")
        
        print(f"\nğŸ“± Articles by Account:")
        for account, count in sorted(accounts.items(), key=lambda x: x[1], reverse=True):
            print(f"  â€¢ {account}: {count} articles")
        
        print(f"\nğŸ”„ Recent Crawl Jobs:")
        for job in recent_jobs:
            status_emoji = {
                "completed": "âœ…",
                "failed": "âŒ",
                "running": "ğŸ”„",
                "pending": "â³"
            }.get(job['status'], "â“")
            
            print(f"{status_emoji} {job['account_name']} - {job['status']} "
                  f"({job['crawled_articles']}/{job['total_articles']} articles)")
        
    finally:
        db.close()


async def list_articles(account_name: Optional[str] = None, limit: int = 20, search: Optional[str] = None):
    """List articles with optional filtering"""
    db = DatabaseManager(use_mongodb=False)
    
    try:
        if search:
            articles = db.search_articles(search, limit)
            print(f"\nğŸ” Search results for '{search}' (showing {len(articles)} results):")
        elif account_name:
            articles = db.get_articles_by_account(account_name, limit)
            print(f"\nğŸ“± Articles from '{account_name}' (showing {len(articles)} results):")
        else:
            articles = db.get_articles_by_account("", limit)
            print(f"\nğŸ“„ All articles (showing {len(articles)} results):")
        
        print("=" * 100)
        
        for i, article in enumerate(articles, 1):
            title = article.get('title', 'No Title')[:60]
            account = article.get('account_name', 'Unknown')
            author = article.get('author', 'Unknown')
            publish_time = article.get('publish_time', 'Unknown')
            url = article.get('url', '')
            
            print(f"{i:3d}. {title}")
            print(f"     ğŸ“± Account: {account} | âœï¸  Author: {author}")
            print(f"     ğŸ“… Published: {publish_time}")
            print(f"     ğŸ”— URL: {url}")
            print()
        
        if not articles:
            print("No articles found.")
        
    finally:
        db.close()


async def show_article_detail(article_id: Optional[int] = None, url: Optional[str] = None):
    """Show detailed information about a specific article"""
    db = DatabaseManager(use_mongodb=False)
    
    try:
        if url:
            article = db.get_article(url)
        elif article_id:
            # This would need to be implemented in the database manager
            print("âŒ Search by ID not yet implemented. Use URL instead.")
            return
        else:
            print("âŒ Please provide either article URL or ID")
            return
        
        if not article:
            print("âŒ Article not found")
            return
        
        print("\n" + "=" * 80)
        print("ğŸ“„ ARTICLE DETAILS")
        print("=" * 80)
        
        print(f"ğŸ“° Title: {article.get('title', 'No Title')}")
        print(f"âœï¸  Author: {article.get('author', 'Unknown')}")
        print(f"ğŸ“± Account: {article.get('account_name', 'Unknown')}")
        print(f"ğŸ“… Published: {article.get('publish_time', 'Unknown')}")
        print(f"ğŸ•’ Crawled: {article.get('crawl_time', 'Unknown')}")
        print(f"ğŸ”— URL: {article.get('url', '')}")
        
        images = article.get('images', [])
        print(f"ğŸ–¼ï¸  Images: {len(images)} found")
        if images:
            for i, img_url in enumerate(images[:3], 1):
                print(f"   {i}. {img_url}")
            if len(images) > 3:
                print(f"   ... and {len(images) - 3} more images")
        
        content = article.get('content', '')
        cleaned_content = _clean_article_content(content)
        
        print(f"\nğŸ“ Content (åŸå§‹: {len(content)} å­—ç¬¦, æ¸…ç†å: {len(cleaned_content)} å­—ç¬¦):")
        print("-" * 80)
        
        if cleaned_content:
            # Show first 1000 characters of cleaned content
            preview = cleaned_content[:1000]
            print(preview)
            if len(cleaned_content) > 1000:
                print(f"\n... (showing first 1000 of {len(cleaned_content)} characters)")
                print(f"ğŸ’¡ Full cleaned content is available")
        else:
            print("No content available after cleaning")
        
        print("\n" + "=" * 80)
        
    finally:
        db.close()


async def import_json_to_database(input_file: str, account_name: Optional[str] = None, 
                                  skip_existing: bool = True, clean_content: bool = True):
    """
    å°†JSONæ–‡ä»¶ä¸­çš„æ•°æ®å¯¼å…¥åˆ°æ•°æ®åº“
    
    Args:
        input_file: JSONæ–‡ä»¶è·¯å¾„
        account_name: æŒ‡å®šè´¦å·åç§°ï¼ˆå¦‚æœJSONä¸­æ²¡æœ‰ï¼‰
        skip_existing: æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„æ–‡ç« ï¼ˆæ ¹æ®URLåˆ¤æ–­ï¼‰
        clean_content: æ˜¯å¦æ¸…ç†æ–‡ç« å†…å®¹
    """
    db = DatabaseManager(use_mongodb=False)
    
    try:
        # è¯»å–JSONæ–‡ä»¶
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        articles_data = data.get('articles', [])
        if not articles_data:
            print(f"âŒ JSONæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°articleså­—æ®µæˆ–ä¸ºç©º")
            return
        
        print(f"ğŸ“„ è¯»å–JSONæ–‡ä»¶: {input_file}")
        print(f"ğŸ“Š å‘ç° {len(articles_data)} æ¡æ•°æ®")
        
        imported_count = 0
        skipped_count = 0
        error_count = 0
        
        # è¾…åŠ©å‡½æ•°å®šä¹‰
        def safe_strip(value):
            """å®‰å…¨åœ°å¤„ç†å­—ç¬¦ä¸²stripï¼Œé¿å…Noneå€¼é”™è¯¯"""
            if value is None:
                return ''
            return str(value).strip()
        
        def parse_content_items(content_text):
            """è§£ææ–‡ç« å†…å®¹ï¼Œæå–ç‹¬ç«‹çš„æ–‡ç« æ¡ç›®"""
            if not content_text:
                return []
            
            import re
            items = []
            
            # ç”¨æ›´æ™ºèƒ½çš„æ–¹å¼åˆ‡åˆ†ï¼šæŒ‰ç…§ "æ ‡é¢˜\nURL\nä½œè€…:\nå†…å®¹" çš„æ¨¡å¼
            url_pattern = r'https?://[^\s]+'
            
            # é¦–å…ˆæ‰¾åˆ°æ‰€æœ‰URLçš„ä½ç½®ï¼Œä½œä¸ºåˆ‡åˆ†ç‚¹
            urls_in_text = list(re.finditer(url_pattern, content_text))
            
            if not urls_in_text:
                return []
            
            # æŒ‰ç…§URLä½ç½®åˆ‡åˆ†å†…å®¹
            for i, url_match in enumerate(urls_in_text):
                # ç¡®å®šå½“å‰æ–‡ç« çš„å¼€å§‹å’Œç»“æŸä½ç½®
                start_pos = 0 if i == 0 else urls_in_text[i-1].end()
                end_pos = urls_in_text[i+1].start() if i+1 < len(urls_in_text) else len(content_text)
                
                # æå–å½“å‰æ–‡ç« çš„å®Œæ•´æ–‡æœ¬å—
                article_block = content_text[start_pos:end_pos].strip()
                
                if not article_block:
                    continue
                
                # åˆ†æè¿™ä¸ªæ–‡æœ¬å—
                url = url_match.group()
                lines = article_block.split('\n')
                
                title = ''
                author = ''
                content_lines = []
                
                url_line_index = -1
                for j, line in enumerate(lines):
                    if url in line:
                        url_line_index = j
                        # URLå‰é¢çš„æ–‡æœ¬å¯èƒ½æ˜¯æ ‡é¢˜
                        url_pos = line.find(url)
                        if url_pos > 0:
                            title = line[:url_pos].strip()
                        break
                
                # å¦‚æœURLè¡Œå‰é¢è¿˜æœ‰è¡Œï¼Œå¯èƒ½æ˜¯æ ‡é¢˜
                if url_line_index > 0 and not title:
                    for j in range(url_line_index-1, -1, -1):
                        line = lines[j].strip()
                        if line and len(line) < 200:  # æ ‡é¢˜ä¸åº”è¯¥å¤ªé•¿
                            title = line
                            break
                
                # URLè¡Œåé¢çš„å†…å®¹
                for j in range(url_line_index + 1, len(lines)):
                    line = lines[j].strip()
                    if not line:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ä½œè€…æ ¼å¼ (Author:)
                    if ':' in line and len(line.split(':', 1)[0].strip()) < 20:
                        parts = line.split(':', 1)
                        potential_author = parts[0].strip()
                        remaining_content = parts[1].strip()
                        
                        # å¦‚æœçœ‹èµ·æ¥åƒä½œè€…å
                        if not re.search(r'https?://', potential_author):
                            author = potential_author
                            if remaining_content:
                                content_lines.append(remaining_content)
                        else:
                            content_lines.append(line)
                    else:
                        content_lines.append(line)
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„æ ‡é¢˜ï¼Œå°è¯•ä»å†…å®¹çš„ç¬¬ä¸€è¡Œæå–
                if not title and content_lines:
                    first_line = content_lines[0]
                    if len(first_line) < 150:  # å¯èƒ½æ˜¯æ ‡é¢˜
                        title = first_line
                        content_lines = content_lines[1:]
                
                content = '\n'.join(content_lines).strip()
                
                if title and url:
                    items.append({
                        'title': title,
                        'url': url,
                        'content': content,
                        'author': author
                    })
            
            return items
        
        for item in articles_data:
            try:
                if 'attributes' in item:
                    # ç»“æ„åŒ–æ ¼å¼ï¼š{id: x, attributes: {...}}
                    attrs = item['attributes']
                    title = safe_strip(attrs.get('title', ''))
                    url = safe_strip(attrs.get('url', ''))
                    author = safe_strip(attrs.get('author', ''))
                    introduce = safe_strip(attrs.get('introduce', ''))
                    full_content = safe_strip(attrs.get('full_content', ''))
                    episode = safe_strip(attrs.get('episode', ''))
                    time_str = safe_strip(attrs.get('time', ''))
                else:
                    # ç®€å•æ ¼å¼ï¼šç›´æ¥æ˜¯æ–‡ç« å­—æ®µ
                    title = safe_strip(item.get('title', ''))
                    url = safe_strip(item.get('url', ''))
                    author = safe_strip(item.get('author', ''))
                    introduce = safe_strip(item.get('introduce', item.get('content', '')))
                    full_content = safe_strip(item.get('full_content', ''))
                    episode = safe_strip(item.get('episode', ''))
                    time_str = safe_strip(item.get('time', item.get('publish_time', '')))
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯åˆé›†æ–‡ç« ï¼ˆåŒ…å«å¤šä¸ªå­æ–‡ç« ï¼‰
                content_to_parse = full_content or introduce
                parsed_items = []
                
                if content_to_parse and (title == "Web3 æå®¢æ—¥æŠ¥" or "æå®¢æ—¥æŠ¥" in title):
                    # è¿™æ˜¯ä¸€ä¸ªåˆé›†æ–‡ç« ï¼Œéœ€è¦æ‹†åˆ†
                    parsed_items = parse_content_items(content_to_parse)
                    print(f"ğŸ“° å‘ç°åˆé›†æ–‡ç« : {title}, æ‹†åˆ†å‡º {len(parsed_items)} ä¸ªå­æ–‡ç« ")
                
                # å¦‚æœæ²¡æœ‰æ‹†åˆ†å‡ºå­æ–‡ç« ï¼Œæˆ–è€…ä¸æ˜¯åˆé›†ï¼Œä¿æŒåŸæ ·
                if not parsed_items:
                    parsed_items = [{
                        'title': title,
                        'url': url,
                        'content': content_to_parse,
                        'author': author
                    }]
                
                # URLæ ¼å¼ä¿®å¤
                def fix_url(url_str):
                    """ä¿®å¤å¸¸è§çš„URLæ ¼å¼é—®é¢˜"""
                    if not url_str:
                        return ''
                    
                    url_str = url_str.strip()
                    
                    # å¤„ç†åŒ…å«å¤šä¸ªURLçš„æƒ…å†µï¼Œå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„
                    if ' ' in url_str and ('http://' in url_str or 'https://' in url_str):
                        parts = url_str.split()
                        for part in parts:
                            if part.startswith(('http://', 'https://')):
                                url_str = part
                                break
                    
                    # å¤„ç†ä¸­æ–‡å‰ç¼€çš„URLï¼ˆå¦‚ï¼šä¸­æ–‡ç‰ˆï¼šhttp://...ï¼‰
                    if 'ï¼šhttp' in url_str:
                        url_str = url_str.split('ï¼šhttp')[1]
                        url_str = 'http' + url_str
                    elif ':http' in url_str:
                        url_str = url_str.split(':http')[1]  
                        url_str = 'http' + url_str
                    
                    # ä¿®å¤å¸¸è§çš„æ‹¼å†™é”™è¯¯
                    url_str = url_str.replace('hu.baittps://', 'https://')
                    url_str = url_str.replace('htps://', 'https://')
                    url_str = url_str.replace('htp://', 'http://')
                    
                    # å¦‚æœæ²¡æœ‰åè®®å‰ç¼€ï¼Œæ·»åŠ https://
                    if url_str and not url_str.startswith(('http://', 'https://', 'ftp://')):
                        # æ£€æŸ¥æ˜¯å¦æ˜¯åŸŸåæ ¼å¼
                        if '.' in url_str and not url_str.startswith('/'):
                            url_str = 'https://' + url_str
                    
                    # ç§»é™¤URLæœ«å°¾çš„å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
                    url_str = url_str.rstrip()
                    
                    # æ£€æŸ¥URLæ˜¯å¦åŒ…å«æ— æ•ˆå­—ç¬¦ï¼ˆç©ºæ ¼ã€ä¸­æ–‡ç­‰ï¼‰
                    if url_str.startswith(('http://', 'https://')):
                        # å¦‚æœåŒ…å«ç©ºæ ¼æˆ–ä¸­æ–‡å­—ç¬¦ï¼Œå¯èƒ½æ˜¯æ— æ•ˆURL
                        import re
                        if re.search(r'[\s\u4e00-\u9fff]', url_str):
                            # å°è¯•æå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„URLéƒ¨åˆ†
                            match = re.search(r'https?://[^\s\u4e00-\u9fff]+', url_str)
                            if match:
                                url_str = match.group(0)
                            else:
                                return ''  # æ— æ³•ä¿®å¤ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
                    
                    return url_str
                
                # è§£æå‘å¸ƒæ—¶é—´
                publish_time = None
                if time_str:
                    try:
                        # å°è¯•å¤šç§æ—¶é—´æ ¼å¼
                        for fmt in ['%Y-%m-%d', '%Y-%m-%d %H:%M:%S', '%Y/%m/%d', '%Yå¹´%mæœˆ%dæ—¥']:
                            try:
                                publish_time = datetime.strptime(time_str, fmt)
                                break
                            except ValueError:
                                continue
                    except:
                        pass
                
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„å‘å¸ƒæ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
                if publish_time is None:
                    publish_time = datetime.now()
                
                # å¤„ç†æ‹†åˆ†åçš„æ¯ä¸ªå­æ–‡ç« 
                for parsed_item in parsed_items:
                    sub_title = parsed_item.get('title', '').strip()
                    sub_url = parsed_item.get('url', '').strip()
                    sub_content = parsed_item.get('content', '').strip()
                    sub_author = parsed_item.get('author', '').strip() or author
                    
                    # ä¿®å¤URL
                    sub_url = fix_url(sub_url)
                    
                    # éªŒè¯å¿…éœ€å­—æ®µ
                    if not sub_title or not sub_url:
                        print(f"âš ï¸  è·³è¿‡æ— æ•ˆå­æ–‡ç«  - ç¼ºå°‘æ ‡é¢˜æˆ–URL: {sub_title[:30] if sub_title else 'No title'}")
                        error_count += 1
                        continue
                    
                    # éªŒè¯URLæ ¼å¼
                    if sub_url and not sub_url.startswith(('http://', 'https://')):
                        print(f"âš ï¸  è·³è¿‡æ— æ•ˆURLæ ¼å¼: {sub_url[:50]}")
                        error_count += 1
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    if skip_existing and db.get_article_by_url(sub_url):
                        skipped_count += 1
                        continue
                    
                    # å‡†å¤‡å†…å®¹
                    final_content = sub_content
                    if clean_content and final_content:
                        final_content = _clean_article_content(final_content)
                    
                    # ä¸ºå­æ–‡ç« ä½¿ç”¨ç¨å¾®ä¸åŒçš„æ—¶é—´ï¼ˆé¿å…å®Œå…¨ç›¸åŒï¼‰
                    sub_publish_time = publish_time
                    if len(parsed_items) > 1:
                        import random
                        # æ·»åŠ éšæœºçš„å‡ åˆ†é’Ÿåç§»
                        offset_minutes = random.randint(1, len(parsed_items) * 2)
                        from datetime import timedelta
                        sub_publish_time = publish_time + timedelta(minutes=offset_minutes)
                    
                    # åˆ›å»ºArticleå¯¹è±¡
                    from storage.models import Article
                    article = Article(
                        url=sub_url,
                        title=sub_title,
                        content=final_content or 'å†…å®¹æš‚æ— ',
                        author=sub_author or 'Unknown',
                        account_name=account_name or sub_author or 'Imported',
                        publish_time=sub_publish_time,
                        images=[],
                        cover_image=None,
                        read_count=0,
                        like_count=0,
                        comment_count=0,
                        raw_html=None,
                        crawl_time=datetime.now()
                    )
                    
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    if db.save_article(article):
                        imported_count += 1
                        print(f"âœ… å¯¼å…¥: {sub_title[:50]}")
                    else:
                        print(f"â„¹ï¸  å·²å­˜åœ¨: {sub_title[:50]}")
                        skipped_count += 1
                    
            except Exception as e:
                error_count += 1
                print(f"âŒ å¯¼å…¥å¤±è´¥: {str(e)}")
                continue
        
        print(f"\nğŸ‰ å¯¼å…¥å®Œæˆï¼")
        print(f"ğŸ“Š å¯¼å…¥ç»Ÿè®¡:")
        print(f"  - æˆåŠŸå¯¼å…¥: {imported_count} æ¡")
        print(f"  - å·²å­˜åœ¨è·³è¿‡: {skipped_count} æ¡") 
        print(f"  - é”™è¯¯æ•°æ®: {error_count} æ¡")
        print(f"  - æ€»å¤„ç†: {len(articles_data)} æ¡")
        
        return imported_count
        
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {input_file}")
        raise
    except json.JSONDecodeError as e:
        print(f"âŒ JSONæ ¼å¼é”™è¯¯: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {str(e)}")
        raise
    finally:
        db.close()


async def delete_articles(keyword: Optional[str] = None, account: Optional[str] = None, 
                          url_pattern: Optional[str] = None, confirm: bool = True):
    """
    åˆ é™¤åŒ¹é…æ¡ä»¶çš„æ–‡ç« 
    
    Args:
        keyword: æ ‡é¢˜å…³é”®è¯åŒ¹é…
        account: è´¦å·åç§°åŒ¹é…
        url_pattern: URLæ¨¡å¼åŒ¹é…
        confirm: æ˜¯å¦éœ€è¦ç¡®è®¤
    """
    db = DatabaseManager(use_mongodb=False)
    
    try:
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        conditions = []
        if keyword:
            conditions.append(f"æ ‡é¢˜åŒ…å« '{keyword}'")
        if account:
            conditions.append(f"è´¦å·ä¸º '{account}'")
        if url_pattern:
            conditions.append(f"URLåŒ…å« '{url_pattern}'")
        
        if not conditions:
            print("âŒ è¯·è‡³å°‘æŒ‡å®šä¸€ä¸ªåˆ é™¤æ¡ä»¶")
            return
        
        print(f"ğŸ” æŸ¥æ‰¾åŒ¹é…ä»¥ä¸‹æ¡ä»¶çš„æ–‡ç« :")
        for condition in conditions:
            print(f"  - {condition}")
        
        # æŸ¥è¯¢åŒ¹é…çš„æ–‡ç« 
        if db.use_mongodb:
            # MongoDBæŸ¥è¯¢é€»è¾‘
            query = {}
            if keyword:
                query["title"] = {"$regex": keyword, "$options": "i"}
            if account:
                query["account_name"] = account
            if url_pattern:
                query["url"] = {"$regex": url_pattern, "$options": "i"}
            
            articles = list(db.articles_collection.find(query))
        else:
            # SQLiteæŸ¥è¯¢é€»è¾‘
            with db.get_session() as session:
                from storage.models import ArticleDB
                from sqlalchemy import and_, or_
                
                query = session.query(ArticleDB)
                
                filter_conditions = []
                if keyword:
                    filter_conditions.append(ArticleDB.title.like(f'%{keyword}%'))
                if account:
                    filter_conditions.append(ArticleDB.account_name == account)
                if url_pattern:
                    filter_conditions.append(ArticleDB.url.like(f'%{url_pattern}%'))
                
                if filter_conditions:
                    query = query.filter(and_(*filter_conditions))
                
                articles = query.all()
                articles = [article.to_dict() for article in articles]
        
        if not articles:
            print("âœ… æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡ç« ")
            return
        
        print(f"\nğŸ“‹ æ‰¾åˆ° {len(articles)} ç¯‡åŒ¹é…çš„æ–‡ç« :")
        for i, article in enumerate(articles[:10], 1):  # æœ€å¤šæ˜¾ç¤º10ç¯‡
            title = article.get('title', '')[:50]
            author = article.get('author', '')
            account_name = article.get('account_name', '')
            url = article.get('url', '')[:50]
            print(f"  {i}. {title}")
            print(f"     ä½œè€…: {author} | è´¦å·: {account_name}")
            print(f"     URL: {url}...")
            print()
        
        if len(articles) > 10:
            print(f"     ... è¿˜æœ‰ {len(articles) - 10} ç¯‡æ–‡ç« ")
        
        # ç¡®è®¤åˆ é™¤
        if confirm:
            print(f"âš ï¸  è­¦å‘Š: æ­¤æ“ä½œå°†æ°¸ä¹…åˆ é™¤ {len(articles)} ç¯‡æ–‡ç« ï¼")
            response = input("ç¡®è®¤åˆ é™¤ï¼Ÿè¾“å…¥ 'yes' ç»§ç»­ï¼Œå…¶ä»–ä»»æ„é”®å–æ¶ˆ: ").strip().lower()
            if response != 'yes':
                print("âŒ åˆ é™¤æ“ä½œå·²å–æ¶ˆ")
                return
        
        # æ‰§è¡Œåˆ é™¤
        deleted_count = 0
        
        if db.use_mongodb:
            # MongoDBåˆ é™¤
            result = db.articles_collection.delete_many(query)
            deleted_count = result.deleted_count
        else:
            # SQLiteåˆ é™¤
            with db.get_session() as session:
                from storage.models import ArticleDB
                
                for article in articles:
                    article_obj = session.query(ArticleDB).filter_by(url=article['url']).first()
                    if article_obj:
                        session.delete(article_obj)
                        deleted_count += 1
        
        print(f"\nğŸ‰ åˆ é™¤å®Œæˆï¼")
        print(f"ğŸ“Š åˆ é™¤ç»Ÿè®¡: æˆåŠŸåˆ é™¤ {deleted_count} ç¯‡æ–‡ç« ")
        
        # æ˜¾ç¤ºåˆ é™¤åçš„ç»Ÿè®¡
        total_remaining = db.get_article_count()
        print(f"ğŸ“ˆ æ•°æ®åº“ç°çŠ¶: å‰©ä½™ {total_remaining} ç¯‡æ–‡ç« ")
        
        return deleted_count
        
    except Exception as e:
        print(f"âŒ åˆ é™¤å¤±è´¥: {str(e)}")
        raise
    finally:
        db.close()


async def clear_database(confirm: bool = True):
    """
    æ¸…ç©ºæ•°æ®åº“ä¸­çš„æ‰€æœ‰æ–‡ç« 
    
    Args:
        confirm: æ˜¯å¦éœ€è¦ç¡®è®¤ï¼ˆé»˜è®¤Trueï¼‰
    """
    db = DatabaseManager(use_mongodb=False)
    
    try:
        # è·å–å½“å‰æ–‡ç« æ•°é‡
        total_count = db.get_article_count()
        
        if total_count == 0:
            print("ğŸ“Š æ•°æ®åº“å·²ç»æ˜¯ç©ºçš„ï¼Œæ— éœ€æ¸…ç†")
            return 0
        
        print(f"ğŸ“Š æ•°æ®åº“å½“å‰åŒ…å« {total_count} ç¯‡æ–‡ç« ")
        
        # ç¡®è®¤åˆ é™¤
        if confirm:
            print(f"âš ï¸  è­¦å‘Š: æ­¤æ“ä½œå°†æ°¸ä¹…åˆ é™¤æ•°æ®åº“ä¸­çš„æ‰€æœ‰ {total_count} ç¯‡æ–‡ç« ï¼")
            response = input("ç¡®è®¤æ¸…ç©ºæ•°æ®åº“ï¼Ÿè¾“å…¥ 'YES' ç»§ç»­ï¼Œå…¶ä»–ä»»æ„é”®å–æ¶ˆ: ").strip()
            if response != 'YES':
                print("âŒ æ¸…ç©ºæ“ä½œå·²å–æ¶ˆ")
                return 0
        
        # æ‰§è¡Œæ¸…ç©º
        deleted_count = 0
        
        if db.use_mongodb:
            # MongoDBåˆ é™¤æ‰€æœ‰
            result = db.articles_collection.delete_many({})
            deleted_count = result.deleted_count
        else:
            # SQLiteåˆ é™¤æ‰€æœ‰
            with db.get_session() as session:
                from storage.models import ArticleDB
                deleted_count = session.query(ArticleDB).delete()
        
        print(f"\nğŸ‰ æ•°æ®åº“æ¸…ç©ºå®Œæˆï¼")
        print(f"ğŸ“Š åˆ é™¤ç»Ÿè®¡: æˆåŠŸåˆ é™¤ {deleted_count} ç¯‡æ–‡ç« ")
        print(f"ğŸ“ˆ æ•°æ®åº“ç°çŠ¶: æ•°æ®åº“å·²æ¸…ç©º")
        
        return deleted_count
        
    except Exception as e:
        print(f"âŒ æ¸…ç©ºæ•°æ®åº“å¤±è´¥: {str(e)}")
        raise
    finally:
        db.close()


async def export_articles(account_name: Optional[str] = None, format_type: str = "txt"):
    """Export articles to file"""
    db = DatabaseManager(use_mongodb=False)
    
    try:
        if account_name:
            articles = db.get_articles_by_account(account_name, limit=1000)
            filename = f"export_{account_name}_{format_type}"
        else:
            articles = db.get_articles_by_account("", limit=1000)
            filename = f"export_all_articles.{format_type}"
        
        if not articles:
            print("âŒ No articles found to export")
            return
        
        # Clean filename
        import re
        filename = re.sub(r'[^\w\-_\.]', '_', filename)
        filepath = f"data/{filename}"
        
        # Create data directory if not exists
        import os
        os.makedirs("data", exist_ok=True)
        
        if format_type == "txt":
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(f"WeChat Articles Export\n")
                f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Total Articles: {len(articles)}\n")
                f.write("=" * 80 + "\n\n")
                
                for i, article in enumerate(articles, 1):
                    # æ¸…ç†æ–‡ç« å†…å®¹
                    original_content = article.get('content', 'No content')
                    cleaned_content = _clean_article_content(original_content)
                    
                    f.write(f"Article {i}\n")
                    f.write(f"Title: {article.get('title', 'No Title')}\n")
                    f.write(f"Author: {article.get('author', 'Unknown')}\n")
                    f.write(f"Account: {article.get('account_name', 'Unknown')}\n")
                    f.write(f"Published: {article.get('publish_time', 'Unknown')}\n")
                    f.write(f"URL: {article.get('url', '')}\n")
                    f.write("-" * 40 + "\n")
                    f.write(f"{cleaned_content}\n")
                    f.write("\n" + "=" * 80 + "\n\n")
        
        elif format_type == "json":
            import json
            
            # è½¬æ¢ä¸ºç»“æ„åŒ–æ ¼å¼
            structured_articles = []
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯Web3æå®¢æ—¥æŠ¥ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
            is_daily = any('æå®¢æ—¥æŠ¥' in article.get('title', '') for article in articles)
            
            if is_daily:
                # Web3æå®¢æ—¥æŠ¥ç‰¹æ®Šå¤„ç†ï¼šæå–æ¯ç¯‡æ–‡ç« ä¸­çš„å¤šä¸ªé¡¹ç›®
                daily_id = 1
                for article_idx, article in enumerate(articles):
                    episode = _extract_episode_from_title(article.get('title', ''))
                    publish_time = article.get('publish_time', '')
                    
                    # æ ¼å¼åŒ–æ—¶é—´
                    if publish_time:
                        try:
                            for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d', '%Yå¹´%mæœˆ%dæ—¥']:
                                try:
                                    dt = datetime.strptime(publish_time.split(' ')[0], fmt)
                                    publish_time = dt.strftime('%Y-%m-%d')
                                    break
                                except:
                                    continue
                        except:
                            pass
                    
                    # æå–æ—¥æŠ¥ä¸­çš„å„ä¸ªé¡¹ç›®
                    items = _extract_daily_items_from_content(article.get('content', ''))
                    
                    if items:
                        # ä¸ºæ¯ä¸ªé¡¹ç›®åˆ›å»ºä¸€ä¸ªæ¡ç›®
                        for item in items:
                            structured_item = {
                                "id": daily_id,
                                "attributes": {
                                    "episode": episode,
                                    "title": item.get('title', ''),
                                    "author": item.get('author', 'Unknown'),
                                    "url": item.get('url', ''),
                                    "time": publish_time,
                                    "introduce": item.get('introduce', '')
                                }
                            }
                            structured_articles.append(structured_item)
                            daily_id += 1
                    else:
                        # å¦‚æœæ— æ³•æå–é¡¹ç›®ï¼Œä½¿ç”¨æ•´ç¯‡æ–‡ç« ä½œä¸ºä¸€ä¸ªæ¡ç›®
                        structured_articles.append(_format_article_to_structured(article, daily_id))
                        daily_id += 1
            else:
                # æ™®é€šæ–‡ç« å¤„ç†
                for idx, article in enumerate(articles, 1):
                    structured_articles.append(_format_article_to_structured(article, idx))
            
            # ä¿å­˜ç»“æ„åŒ–æ•°æ®
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump({
                    "export_info": {
                        "total": len(structured_articles),
                        "export_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        "account": account_name or "all"
                    },
                    "articles": structured_articles
                }, f, ensure_ascii=False, indent=2, default=str)
            
            # é¢å¤–ä¿å­˜ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼ˆä¸å«full_contentï¼‰
            simplified_filepath = filepath.replace('.json', '_simplified.json')
            simplified_articles = []
            for article in structured_articles:
                simplified = article.copy()
                if 'attributes' in simplified and 'full_content' in simplified['attributes']:
                    del simplified['attributes']['full_content']
                simplified_articles.append(simplified)
            
            with open(simplified_filepath, "w", encoding="utf-8") as f:
                json.dump({
                    "export_info": {
                        "total": len(simplified_articles),
                        "export_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        "account": account_name or "all"
                    },
                    "articles": simplified_articles
                }, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"ğŸ“„ é¢å¤–ä¿å­˜ç®€åŒ–ç‰ˆæœ¬åˆ°: {simplified_filepath}")
        
        elif format_type == "csv":
            import csv
            # æ¸…ç†æ‰€æœ‰æ–‡ç« å†…å®¹
            cleaned_articles = []
            for article in articles:
                cleaned_article = article.copy()
                if 'content' in cleaned_article:
                    cleaned_article['content'] = _clean_article_content(cleaned_article['content'])
                cleaned_articles.append(cleaned_article)
            
            with open(filepath, "w", newline="", encoding="utf-8") as f:
                if cleaned_articles:
                    writer = csv.DictWriter(f, fieldnames=cleaned_articles[0].keys())
                    writer.writeheader()
                    writer.writerows(cleaned_articles)
        
        print(f"âœ… Exported {len(articles)} articles to: {filepath}")
        
    finally:
        db.close()


async def handle_analytics_command(args):
    """å¤„ç†analyticsç›¸å…³å‘½ä»¤"""
    
    def save_results_if_needed(results, output_file, description):
        """å¦‚æœæŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶ï¼Œä¿å­˜ç»“æœ"""
        if output_file:
            os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else 'data', exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            print(f"ğŸ“„ {description}ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    try:
        if args.analytics_command == 'trends':
            print(f"ğŸ” åˆ†ææœ€è¿‘ {args.days} å¤©çš„æŠ€æœ¯è¶‹åŠ¿...")
            analyzer = TrendAnalyzer()
            results = analyzer.analyze_technology_trends(args.days)
            
            if "error" not in results:
                print(f"\nğŸ“Š æŠ€æœ¯è¶‹åŠ¿åˆ†æç»“æœ (æœ€è¿‘ {args.days} å¤©)")
                print("=" * 60)
                print(f"ğŸ“„ åˆ†ææ–‡ç« æ•°: {results['total_articles']}")
                print(f"ğŸ·ï¸  å‘ç°å…³é”®è¯: {results['total_keywords']}")
                print(f"ğŸ” çƒ­é—¨æŠ€æœ¯è¶‹åŠ¿ (å‰10å):")
                
                for i, trend in enumerate(results['top_trends'][:10], 1):
                    print(f"  {i:2d}. {trend['keyword']:<20} - {trend['count']:3d} æ¬¡ ({trend['percentage']:5.1f}%)")
                
                save_results_if_needed(results, args.output, "æŠ€æœ¯è¶‹åŠ¿åˆ†æ")
            else:
                print(f"âŒ åˆ†æå¤±è´¥: {results['error']}")
        
        elif args.analytics_command == 'authors':
            print(f"ğŸ‘¥ åˆ†ææœ€è¿‘ {args.days} å¤©çš„ä½œè€…æ´»è·ƒåº¦...")
            analyzer = TrendAnalyzer()
            results = analyzer.analyze_author_activity(args.days)
            
            if "error" not in results:
                print(f"\nğŸ“Š ä½œè€…æ´»è·ƒåº¦åˆ†æç»“æœ (æœ€è¿‘ {args.days} å¤©)")
                print("=" * 60)
                print(f"ğŸ‘¨â€ğŸ’» æ´»è·ƒä½œè€…æ•°: {results['total_authors']}")
                print(f"ğŸ“„ åˆ†ææ–‡ç« æ•°: {results['total_articles']}")
                print(f"ğŸ† å½±å“åŠ›æ’è¡Œæ¦œ (å‰10å):")
                
                for i, author in enumerate(results['top_authors'][:10], 1):
                    print(f"  {i:2d}. {author['author']:<20} - {author['article_count']:2d} ç¯‡ "
                          f"(å½±å“åŠ›: {author['influence_score']:5.1f}, æ—¥å‡: {author['productivity']:4.2f})")
                    print(f"      å…³é”®è¯: {', '.join(author['top_keywords'])}")
                
                print(f"\nğŸ“ˆ ä½œè€…åˆ†å¸ƒ:")
                dist = results['author_distribution']
                print(f"  â€¢ é«˜äº§ä½œè€… (â‰¥5ç¯‡): {dist['highly_active']} äºº")
                print(f"  â€¢ ä¸­äº§ä½œè€… (2-4ç¯‡): {dist['moderately_active']} äºº")
                print(f"  â€¢ å¶å‘ä½œè€… (1ç¯‡): {dist['occasionally_active']} äºº")
                
                save_results_if_needed(results, args.output, "ä½œè€…æ´»è·ƒåº¦åˆ†æ")
            else:
                print(f"âŒ åˆ†æå¤±è´¥: {results['error']}")
        
        elif args.analytics_command == 'publishing':
            print(f"ğŸ“… åˆ†ææœ€è¿‘ {args.days} å¤©çš„å‘å¸ƒæ¨¡å¼...")
            analyzer = TrendAnalyzer()
            results = analyzer.analyze_publication_patterns(args.days)
            
            if "error" not in results:
                print(f"\nğŸ“Š å‘å¸ƒæ¨¡å¼åˆ†æç»“æœ (æœ€è¿‘ {args.days} å¤©)")
                print("=" * 60)
                print(f"ğŸ“„ åˆ†ææ–‡ç« æ•°: {results['total_articles']}")
                
                daily_stats = results['daily_statistics']
                print(f"ğŸ“ˆ æ—¥å‘å¸ƒç»Ÿè®¡:")
                print(f"  â€¢ æ—¥å‡å‘å¸ƒ: {daily_stats['average']} ç¯‡")
                print(f"  â€¢ æœ€é«˜å•æ—¥: {daily_stats['maximum']} ç¯‡")
                print(f"  â€¢ æœ€ä½å•æ—¥: {daily_stats['minimum']} ç¯‡")
                print(f"  â€¢ æœ€æ´»è·ƒæ—¥: {daily_stats['most_active_day']['date']} ({daily_stats['most_active_day']['count']} ç¯‡)")
                
                temporal = results['temporal_patterns']
                print(f"â° æ—¶é—´æ¨¡å¼:")
                print(f"  â€¢ æœ€æ´»è·ƒæ—¶æ®µ: {temporal['most_active_hour']['hour']:02d}:00 ({temporal['most_active_hour']['count']} ç¯‡)")
                print(f"  â€¢ æœ€æ´»è·ƒæœˆä»½: {temporal['most_active_month']['month']} ({temporal['most_active_month']['count']} ç¯‡)")
                
                summary = results['distribution_summary']
                print(f"ğŸ“Š è¦†ç›–ç»Ÿè®¡:")
                print(f"  â€¢ æœ‰æ–‡ç« çš„å¤©æ•°: {summary['days_with_articles']} å¤©")
                print(f"  â€¢ æ´»è·ƒå‘¨æ•°: {summary['active_weeks']} å‘¨")
                print(f"  â€¢ è¦†ç›–ç‡: {summary['coverage_rate']}%")
                
                save_results_if_needed(results, args.output, "å‘å¸ƒæ¨¡å¼åˆ†æ")
            else:
                print(f"âŒ åˆ†æå¤±è´¥: {results['error']}")
        
        elif args.analytics_command == 'report':
            print(f"ğŸ“‹ ç”Ÿæˆç»¼åˆæ•°æ®åˆ†ææŠ¥å‘Š (æœ€è¿‘ {args.days} å¤©)...")
            analyzer = TrendAnalyzer()
            results = analyzer.get_comprehensive_trends(args.days)
            
            if "error" not in results:
                summary = results['summary']
                print(f"\nğŸ“Š ç»¼åˆæ•°æ®åˆ†ææŠ¥å‘Š")
                print("=" * 60)
                print(f"ğŸ“„ åˆ†ææ–‡ç« æ€»æ•°: {summary['total_articles_analyzed']}")
                print(f"ğŸ‘¥ æ´»è·ƒä½œè€…æ•°é‡: {summary['total_authors']}")
                print(f"ğŸ”¥ æœ€çƒ­é—¨æŠ€æœ¯: {summary['most_discussed_tech']}")
                print(f"ğŸ† æœ€é«˜äº§ä½œè€…: {summary['most_productive_author']}")
                print(f"ğŸ“ˆ æ—¥å‡å‘å¸ƒé‡: {summary['daily_average']}")
                
                print(f"\nğŸ’¡ è¯¦ç»†åˆ†ææ•°æ®å·²åŒ…å«åœ¨å¯¼å‡ºç»“æœä¸­")
                save_results_if_needed(results, args.output, "ç»¼åˆåˆ†ææŠ¥å‘Š")
            else:
                print(f"âŒ åˆ†æå¤±è´¥: {results['error']}")
        
        elif args.analytics_command == 'tags':
            if args.tag_command == 'extract':
                print(f"ğŸ·ï¸  å¼€å§‹æ™ºèƒ½æ ‡ç­¾æå–...")
                if args.limit:
                    print(f"ğŸ“„ å¤„ç†æ–‡ç« æ•°é™åˆ¶: {args.limit}")
                
                extractor = TagExtractor()
                results = extractor.batch_tag_articles(limit=args.limit)
                
                if "error" not in results:
                    summary = results['summary']
                    print(f"\nğŸ“Š æ ‡ç­¾æå–ç»“æœ")
                    print("=" * 60)
                    print(f"ğŸ“„ å¤„ç†æ–‡ç« æ•°: {summary['total_articles_processed']}")
                    print(f"âœ… æˆåŠŸæ ‡è®°æ•°: {summary['successfully_tagged']}")
                    
                    print(f"\nğŸ·ï¸  çƒ­é—¨æ ‡ç­¾åˆ†ç±»:")
                    for category, tags in summary['most_common_tags'].items():
                        if tags:
                            print(f"  {category}:")
                            for tag, count in list(tags.items())[:5]:
                                print(f"    â€¢ {tag}: {count} æ¬¡")
                    
                    save_results_if_needed(results, args.output, "æ ‡ç­¾æå–")
                else:
                    print(f"âŒ æ ‡ç­¾æå–å¤±è´¥: {results['error']}")
            
            elif args.tag_command == 'trends':
                print(f"ğŸ“ˆ åˆ†ææœ€è¿‘ {args.days} å¤©çš„æ ‡ç­¾è¶‹åŠ¿...")
                extractor = TagExtractor()
                results = extractor.analyze_tag_trends(args.days)
                
                if "error" not in results:
                    print(f"\nğŸ“Š æ ‡ç­¾è¶‹åŠ¿åˆ†æç»“æœ (æœ€è¿‘ {args.days} å¤©)")
                    print("=" * 60)
                    print(f"ğŸ“„ åˆ†ææ–‡ç« æ•°: {results['total_articles']}")
                    
                    summary = results['summary']
                    print(f"ğŸ”¥ æœ€çƒ­é—¨æ ‡ç­¾ç±»åˆ«: {summary['most_popular_category']}")
                    print(f"ğŸ·ï¸  ç‹¬ç‰¹æ ‡ç­¾æ€»æ•°: {summary['total_unique_tags']}")
                    print(f"ğŸ“Š å¹³å‡æ¯ç¯‡æ ‡ç­¾: {summary['average_tags_per_article']}")
                    
                    print(f"\nğŸ“ˆ çƒ­é—¨æ ‡ç­¾åˆ†å¸ƒ:")
                    for category, trends in results['trending_tags'].items():
                        if trends:
                            print(f"  {category}:")
                            for trend in trends[:3]:
                                print(f"    â€¢ {trend['tag']}: {trend['count']} æ¬¡ ({trend['percentage']}%)")
                    
                    save_results_if_needed(results, args.output, "æ ‡ç­¾è¶‹åŠ¿åˆ†æ")
                else:
                    print(f"âŒ æ ‡ç­¾è¶‹åŠ¿åˆ†æå¤±è´¥: {results['error']}")
            
            else:
                print("âŒ è¯·æŒ‡å®šæ ‡ç­¾å­å‘½ä»¤: extract æˆ– trends")
        
        elif args.analytics_command == 'quality':
            if args.quality_command == 'evaluate':
                print(f"ğŸ“ å¼€å§‹å†…å®¹è´¨é‡è¯„ä¼°...")
                if args.limit:
                    print(f"ğŸ“„ è¯„ä¼°æ–‡ç« æ•°é™åˆ¶: {args.limit}")
                
                evaluator = ContentEvaluator()
                results = evaluator.batch_evaluate_quality(limit=args.limit)
                
                if "error" not in results:
                    summary = results['summary']
                    print(f"\nğŸ“Š å†…å®¹è´¨é‡è¯„ä¼°ç»“æœ")
                    print("=" * 60)
                    print(f"ğŸ“„ è¯„ä¼°æ–‡ç« æ•°: {summary['successfully_evaluated']}")
                    
                    quality_dist = summary['quality_distribution']
                    print(f"ğŸ† è´¨é‡ç­‰çº§åˆ†å¸ƒ:")
                    print(f"  â€¢ Açº§ (ä¼˜ç§€): {quality_dist['A']} ç¯‡")
                    print(f"  â€¢ Bçº§ (è‰¯å¥½): {quality_dist['B']} ç¯‡")
                    print(f"  â€¢ Cçº§ (ä¸€èˆ¬): {quality_dist['C']} ç¯‡")
                    print(f"  â€¢ Dçº§ (å¾…æ”¹è¿›): {quality_dist['D']} ç¯‡")
                    
                    insights = summary['quality_insights']
                    print(f"\nğŸ“ˆ è´¨é‡æ´å¯Ÿ:")
                    print(f"  â€¢ é«˜è´¨é‡ç‡: {insights['high_quality_rate']}%")
                    print(f"  â€¢ éœ€æ”¹è¿›ç‡: {insights['needs_improvement_rate']}%")
                    print(f"  â€¢ å¹³å‡å­—æ•°: {insights['average_word_count']} å­—")
                    print(f"  â€¢ å¹³å‡é˜…è¯»æ—¶é—´: {insights['average_reading_time']} åˆ†é’Ÿ")
                    
                    avg_metrics = summary['average_metrics']
                    print(f"\nğŸ“Š å¹³å‡è´¨é‡æŒ‡æ ‡:")
                    print(f"  â€¢ ç»¼åˆè¯„åˆ†: {avg_metrics['overall']:.3f}")
                    print(f"  â€¢ åŸåˆ›æ€§: {avg_metrics['originality']:.3f}")
                    print(f"  â€¢ æŠ€æœ¯æ·±åº¦: {avg_metrics['technical_depth']:.3f}")
                    print(f"  â€¢ å¯è¯»æ€§: {avg_metrics['readability']:.3f}")
                    print(f"  â€¢ ç»“æ„åŒ–: {avg_metrics['structure']:.3f}")
                    
                    save_results_if_needed(results, args.output, "å†…å®¹è´¨é‡è¯„ä¼°")
                else:
                    print(f"âŒ è´¨é‡è¯„ä¼°å¤±è´¥: {results['error']}")
            
            elif args.quality_command == 'insights':
                print(f"ğŸ’¡ ç”Ÿæˆå†…å®¹è´¨é‡æ´å¯ŸæŠ¥å‘Š...")
                evaluator = ContentEvaluator()
                results = evaluator.get_quality_insights(min_quality_score=args.min_score)
                
                if "error" not in results:
                    print(f"\nğŸ“Š å†…å®¹è´¨é‡æ´å¯ŸæŠ¥å‘Š")
                    print("=" * 60)
                    
                    if 'quality_insights' in results:
                        insights = results['quality_insights']
                        if 'high_quality_characteristics' in insights:
                            chars = insights['high_quality_characteristics']
                            print(f"ğŸ† é«˜è´¨é‡æ–‡ç« ç‰¹å¾ (â‰¥{args.min_score} åˆ†):")
                            print(f"  â€¢ æ ·æœ¬æ•°é‡: {chars['sample_count']} ç¯‡")
                            print(f"  â€¢ å¹³å‡å­—æ•°: {chars['average_word_count']} å­—")
                            print(f"  â€¢ å…±åŒç‰¹å¾:")
                            for pattern in chars.get('common_patterns', []):
                                print(f"    - {pattern}")
                        
                        if 'improvement_recommendations' in insights:
                            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
                            for rec in insights['improvement_recommendations']:
                                print(f"  â€¢ {rec}")
                    
                    save_results_if_needed(results, args.output, "è´¨é‡æ´å¯ŸæŠ¥å‘Š")
                else:
                    print(f"âŒ è´¨é‡æ´å¯Ÿåˆ†æå¤±è´¥: {results['error']}")
            
            else:
                print("âŒ è¯·æŒ‡å®šè´¨é‡åˆ†æå­å‘½ä»¤: evaluate æˆ– insights")
        
        else:
            print("âŒ è¯·æŒ‡å®šanalyticså­å‘½ä»¤: trends, authors, publishing, report, tags, æˆ– quality")
    
    except Exception as e:
        logger.error(f"Analyticså‘½ä»¤æ‰§è¡Œå¤±è´¥: {str(e)}")
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {str(e)}")


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description="WeChat Public Account Crawler")
    subparsers = parser.add_subparsers(dest='command', help='Commands')
    
    # Crawl account command
    crawl_parser = subparsers.add_parser('crawl', help='Crawl a WeChat public account')
    crawl_parser.add_argument('account', help='WeChat public account name')
    crawl_parser.add_argument('--max-articles', type=int, help='Maximum number of articles to crawl')
    crawl_parser.add_argument('--use-proxy', action='store_true', help='Use proxy rotation')
    
    # Crawl single article command
    article_parser = subparsers.add_parser('article', help='Crawl a single article')
    article_parser.add_argument('url', help='Article URL')
    article_parser.add_argument('--show-content', action='store_true', help='Display article content in console')
    
    # Test command
    test_parser = subparsers.add_parser('test', help='Test crawler with predefined article')
    test_parser.add_argument('--show-content', action='store_true', help='Display article content in console')
    
    # Discover from article command
    discover_parser = subparsers.add_parser('discover', help='Discover and crawl articles starting from a single article')
    discover_parser.add_argument('url', help='WeChat article URL to start discovery from')
    discover_parser.add_argument('--max-articles', type=int, help='Maximum number of articles to discover and crawl')
    discover_parser.add_argument('--use-proxy', action='store_true', help='Use proxy rotation')
    
    # History articles command
    history_parser = subparsers.add_parser('history', help='Get history articles from a WeChat account starting from one article')
    history_parser.add_argument('url', help='WeChat article URL to start from')
    history_parser.add_argument('--max-articles', type=int, help='Maximum number of history articles to get')
    history_parser.add_argument('--use-proxy', action='store_true', help='Use proxy rotation')
    
    # Series articles command
    series_parser = subparsers.add_parser('series', help='Get all articles from a WeChat series/album (most effective for series)')
    series_parser.add_argument('url', help='WeChat series article URL')
    series_parser.add_argument('--max-articles', type=int, help='Maximum number of series articles to get')
    series_parser.add_argument('--use-proxy', action='store_true', help='Use proxy rotation')
    
    # Stats command
    stats_parser = subparsers.add_parser('stats', help='Show crawler statistics')
    
    # List articles command
    list_parser = subparsers.add_parser('list', help='List articles with optional filtering')
    list_parser.add_argument('--account', help='Filter by account name')
    list_parser.add_argument('--limit', type=int, default=20, help='Number of articles to show (default: 20)')
    list_parser.add_argument('--search', help='Search articles by keyword')
    
    # Show article detail command
    detail_parser = subparsers.add_parser('detail', help='Show detailed information about a specific article')
    detail_parser.add_argument('--url', help='Article URL')
    detail_parser.add_argument('--id', type=int, help='Article ID')
    
    # Export articles command
    export_parser = subparsers.add_parser('export', help='Export articles to file')
    export_parser.add_argument('--account', help='Export articles from specific account')
    export_parser.add_argument('--format', choices=['txt', 'json', 'csv', 'markdown', 'simple'], default='txt', help='Export format (default: txt)')
    export_parser.add_argument('--output', '-o', help='Output directory for markdown export')
    export_parser.add_argument('--limit', '-l', type=int, help='Limit number of articles to export')
    export_parser.add_argument('--no-category', action='store_true', help='Do not export by category (markdown only)')
    export_parser.add_argument('--no-date', action='store_true', help='Do not export by date (markdown only)')
    export_parser.add_argument('--no-author', action='store_true', help='Do not export by author (markdown only)')
    
    # Import JSON to database command
    import_parser = subparsers.add_parser('import', help='Import JSON data to database')
    import_parser.add_argument('file', help='JSON file to import')
    import_parser.add_argument('--account', help='Specify account name if not in JSON data')
    import_parser.add_argument('--force', action='store_true', help='Import even if articles already exist (update existing)')
    import_parser.add_argument('--no-clean', action='store_true', help='Do not clean article content')
    
    # Clear database command
    clear_parser = subparsers.add_parser('clear', help='Clear all articles from database')
    clear_parser.add_argument('--force', action='store_true', help='Skip confirmation prompt')
    
    # Fetch API data command
    fetch_parser = subparsers.add_parser('fetch', help='Fetch all data from API and save to local JSON file')
    fetch_parser.add_argument('--api-url', help='API endpoint URL (default: http://101.33.75.240:1337/api/v1/geekdailies)')
    fetch_parser.add_argument('--max-pages', type=int, help='Maximum number of pages to fetch (default: all)')
    fetch_parser.add_argument('--output', help='Output file path (default: data/api_geekdailies_TIMESTAMP.json)')
    
    # Merge JSON files command
    merge_parser = subparsers.add_parser('merge', help='Merge two JSON files and remove duplicates')
    merge_parser.add_argument('file1', help='First JSON file to merge')
    merge_parser.add_argument('file2', help='Second JSON file to merge')
    merge_parser.add_argument('--output', help='Output file path (default: data/merged_articles_TIMESTAMP.json)')
    merge_parser.add_argument('--strategy', choices=['url', 'title', 'episode', 'title_author', 'url_title'], 
                             default='url', help='Deduplication strategy (default: url)')
    merge_parser.add_argument('--no-split', action='store_true', help='Disable smart splitting of collection articles')
    merge_parser.add_argument('--no-filter', action='store_true', help='Disable filtering of bad data')
    merge_parser.add_argument('--no-fill-time', action='store_true', help='Disable automatic time filling')
    
    # Analyze JSON files command
    analyze_parser = subparsers.add_parser('analyze', help='Analyze JSON files structure and content')
    analyze_parser.add_argument('files', nargs='+', help='JSON files to analyze')
    
    # Clean JSON data command
    clean_parser = subparsers.add_parser('clean', help='Clean JSON data by removing incomplete entries')
    clean_parser.add_argument('input_file', help='Input JSON file to clean')
    clean_parser.add_argument('--output', help='Output file path (default: INPUT_cleaned_TIMESTAMP.json)')
    clean_parser.add_argument('--require-fields', nargs='+', default=['author', 'url', 'title'],
                             help='Required fields that must be non-empty (default: author url title)')
    clean_parser.add_argument('--no-empty-check', action='store_true', 
                             help='Disable additional empty value checks')
    clean_parser.add_argument('--validate-urls', action='store_true',
                             help='Enable URL format validation')
    clean_parser.add_argument('--exclude-fields', nargs='+', default=[],
                             help='Fields to exclude from output (e.g., episode time)')
    
    # Batch clean command
    batch_clean_parser = subparsers.add_parser('batch-clean', help='Batch clean all JSON files in a directory')
    batch_clean_parser.add_argument('directory', help='Directory containing JSON files')
    batch_clean_parser.add_argument('--pattern', default='*.json', help='File pattern to match (default: *.json)')
    batch_clean_parser.add_argument('--require-fields', nargs='+', default=['author', 'url', 'title'],
                                   help='Required fields that must be non-empty (default: author url title)')
    batch_clean_parser.add_argument('--no-empty-check', action='store_true',
                                   help='Disable additional empty value checks')
    batch_clean_parser.add_argument('--validate-urls', action='store_true',
                                   help='Enable URL format validation')
    batch_clean_parser.add_argument('--exclude-fields', nargs='+', default=[],
                                   help='Fields to exclude from output (e.g., episode time)')
    
    # Web API server command
    api_parser = subparsers.add_parser('api', help='Start web API server for data access')
    api_parser.add_argument('--host', default='127.0.0.1', help='Host address (default: 127.0.0.1)')
    api_parser.add_argument('--port', type=int, default=8000, help='Port number (default: 8000)')
    api_parser.add_argument('--reload', action='store_true', help='Enable auto-reload for development')
    
    # Delete articles command
    delete_parser = subparsers.add_parser('delete', help='Delete articles from database')
    delete_parser.add_argument('--keyword', help='Delete articles with title containing keyword')
    delete_parser.add_argument('--account', help='Delete articles from specific account')
    delete_parser.add_argument('--url-pattern', help='Delete articles with URL containing pattern')
    delete_parser.add_argument('--force', action='store_true', help='Skip confirmation prompt')
    
    # Analytics commands
    analytics_parser = subparsers.add_parser('analytics', help='Data analytics and insights')
    analytics_subparsers = analytics_parser.add_subparsers(dest='analytics_command', help='Analytics commands')
    
    # Trend analysis command
    trend_parser = analytics_subparsers.add_parser('trends', help='Analyze technology trends')
    trend_parser.add_argument('--days', type=int, default=30, help='Analysis period in days (default: 30)')
    trend_parser.add_argument('--output', help='Save results to JSON file')
    
    # Author activity command
    author_parser = analytics_subparsers.add_parser('authors', help='Analyze author activity and influence')
    author_parser.add_argument('--days', type=int, default=30, help='Analysis period in days (default: 30)')
    author_parser.add_argument('--output', help='Save results to JSON file')
    
    # Publication patterns command
    publish_parser = analytics_subparsers.add_parser('publishing', help='Analyze publication patterns and frequency')
    publish_parser.add_argument('--days', type=int, default=90, help='Analysis period in days (default: 90)')
    publish_parser.add_argument('--output', help='Save results to JSON file')
    
    # Comprehensive report command
    report_parser = analytics_subparsers.add_parser('report', help='Generate comprehensive analytics report')
    report_parser.add_argument('--days', type=int, default=30, help='Analysis period in days (default: 30)')
    report_parser.add_argument('--output', help='Save report to JSON file')
    
    # Tag analysis command
    tag_parser = analytics_subparsers.add_parser('tags', help='Intelligent tagging and tag analysis')
    tag_subparsers = tag_parser.add_subparsers(dest='tag_command', help='Tag commands')
    
    # Extract tags command
    extract_tags_parser = tag_subparsers.add_parser('extract', help='Extract tags from articles')
    extract_tags_parser.add_argument('--limit', type=int, help='Limit number of articles to process')
    extract_tags_parser.add_argument('--output', help='Save results to JSON file')
    
    # Tag trends command
    tag_trends_parser = tag_subparsers.add_parser('trends', help='Analyze tag trends')
    tag_trends_parser.add_argument('--days', type=int, default=30, help='Analysis period in days (default: 30)')
    tag_trends_parser.add_argument('--output', help='Save results to JSON file')
    
    # Quality analysis command
    quality_parser = analytics_subparsers.add_parser('quality', help='Content quality assessment')
    quality_subparsers = quality_parser.add_subparsers(dest='quality_command', help='Quality commands')
    
    # Evaluate quality command
    eval_quality_parser = quality_subparsers.add_parser('evaluate', help='Evaluate content quality')
    eval_quality_parser.add_argument('--limit', type=int, help='Limit number of articles to evaluate')
    eval_quality_parser.add_argument('--output', help='Save results to JSON file')
    
    # Quality insights command
    insights_parser = quality_subparsers.add_parser('insights', help='Get quality insights and recommendations')
    insights_parser.add_argument('--min-score', type=float, default=0.7, help='Minimum quality score for high-quality analysis (default: 0.7)')
    insights_parser.add_argument('--output', help='Save results to JSON file')
    
    args = parser.parse_args()
    
    if args.command == 'crawl':
        asyncio.run(crawl_account(
            args.account,
            max_articles=args.max_articles,
            use_proxy=args.use_proxy
        ))
    elif args.command == 'article':
        asyncio.run(crawl_single_article(args.url, show_content=args.show_content))
    elif args.command == 'test':
        asyncio.run(test_crawler(show_content=args.show_content))
    elif args.command == 'discover':
        asyncio.run(discover_from_article(
            args.url,
            max_articles=args.max_articles,
            use_proxy=args.use_proxy
        ))
    elif args.command == 'history':
        asyncio.run(get_history_articles(
            args.url,
            max_articles=args.max_articles,
            use_proxy=args.use_proxy
        ))
    elif args.command == 'series':
        asyncio.run(get_series_articles(
            args.url,
            max_articles=args.max_articles,
            use_proxy=args.use_proxy
        ))
    elif args.command == 'stats':
        asyncio.run(show_stats())
    elif args.command == 'list':
        asyncio.run(list_articles(
            account_name=args.account,
            limit=args.limit,
            search=args.search
        ))
    elif args.command == 'detail':
        asyncio.run(show_article_detail(
            article_id=args.id,
            url=args.url
        ))
    elif args.command == 'export':
        if args.format == 'markdown':
            # ä½¿ç”¨æ–°çš„ Markdown å¯¼å‡ºå™¨
            from export_to_markdown import MarkdownExporter
            
            output_dir = args.output or 'export/markdown'
            exporter = MarkdownExporter(output_dir=output_dir)
            
            exporter.export_all(
                by_category=not args.no_category,
                by_date=not args.no_date,
                by_author=not args.no_author,
                limit=args.limit
            )
        elif args.format == 'simple':
            # ä½¿ç”¨ç®€å•åˆ—è¡¨å¯¼å‡ºå™¨
            from export_simple_list import SimpleListExporter
            
            output_file = args.output or 'all_articles.md'
            exporter = SimpleListExporter()
            
            exporter.export_to_single_file(
                output_file=output_file,
                limit=args.limit
            )
        else:
            asyncio.run(export_articles(
                account_name=args.account,
                format_type=args.format
            ))
    elif args.command == 'import':
        asyncio.run(import_json_to_database(
            input_file=args.file,
            account_name=args.account,
            skip_existing=not args.force,
            clean_content=not args.no_clean
        ))
    elif args.command == 'clear':
        asyncio.run(clear_database(
            confirm=not args.force
        ))
    elif args.command == 'fetch':
        asyncio.run(fetch_api_data(
            api_url=args.api_url,
            max_pages=args.max_pages,
            output_file=args.output
        ))
    elif args.command == 'merge':
        asyncio.run(merge_json_files(
            file1=args.file1,
            file2=args.file2,
            output_file=args.output,
            dedup_strategy=args.strategy,
            smart_split=not args.no_split,
            filter_bad_data=not args.no_filter,
            fill_time=not args.no_fill_time
        ))
    elif args.command == 'analyze':
        asyncio.run(analyze_json_files(*args.files))
    elif args.command == 'clean':
        asyncio.run(clean_json_data(
            input_file=args.input_file,
            output_file=args.output,
            require_fields=args.require_fields,
            clean_empty=not args.no_empty_check,
            validate_urls=args.validate_urls,
            exclude_fields=args.exclude_fields
        ))
    elif args.command == 'batch-clean':
        asyncio.run(batch_clean_directory(
            directory=args.directory,
            pattern=args.pattern,
            require_fields=args.require_fields,
            clean_empty=not args.no_empty_check,
            validate_urls=args.validate_urls,
            exclude_fields=args.exclude_fields
        ))
    elif args.command == 'api':
        from web_api import run_server
        run_server(
            host=args.host,
            port=args.port,
            reload=args.reload
        )
    elif args.command == 'delete':
        asyncio.run(delete_articles(
            keyword=args.keyword,
            account=args.account,
            url_pattern=args.url_pattern,
            confirm=not args.force
        ))
    elif args.command == 'analytics':
        asyncio.run(handle_analytics_command(args))
    else:
        parser.print_help()


if __name__ == "__main__":
    main()